{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bibliographic-documentation",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "### Natural Language ToolKit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e704a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# nltk.download()       # to download all the packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "applied-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Example sentance\n",
    "\n",
    "sent =\"Hey This is me . i am looking for you since past year . where are you now a days? how's your brother ? i am helping you .he steped out from camping . i seen many bridges . nothing can be done . hounorable rahul is honest boy . america has newyork state . elon is richest among all and he owns tesla and great spacex . he studied from precious stanford university . The big brown dog\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-composite",
   "metadata": {},
   "source": [
    "### Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "federal-german",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey This is me .',\n",
       " 'i am looking for you since past year .',\n",
       " 'where are you now a days?',\n",
       " \"how's your brother ?\",\n",
       " 'i am helping you .he steped out from camping .',\n",
       " 'i seen many bridges .',\n",
       " 'nothing can be done .',\n",
       " 'hounorable rahul is honest boy .',\n",
       " 'america has newyork state .',\n",
       " 'elon is richest among all and he owns tesla and great spacex .',\n",
       " 'he studied from precious stanford university .',\n",
       " 'The big brown dog']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import  sent_tokenize\n",
    "\n",
    "st = sent_tokenize(sent)\n",
    "\n",
    "st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-retirement",
   "metadata": {},
   "source": [
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "individual-university",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looking', 'for', 'you', 'since', 'past', 'year', '.', 'where', 'are', 'you']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wt = word_tokenize(sent[21:70])\n",
    "\n",
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "moved-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def a(se):\n",
    "#    return  word_tokenize(se) \n",
    "\n",
    "# list(map(a,sent_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-graduation",
   "metadata": {},
   "source": [
    "## Symbols Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "academic-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sym_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "wt_s = sym_tokenizer.tokenize(sent)    # returns list like word tokenizer without any punctuations and symbols like . , ' ? / | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-equipment",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "injured-paris",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'This'), ('This', 'is'), ('is', 'me'), ('me', 'i'), ('i', 'am')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams =  list(nltk.bigrams(wt_s))\n",
    "bigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "supported-negative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'This', 'is'),\n",
       " ('This', 'is', 'me'),\n",
       " ('is', 'me', 'i'),\n",
       " ('me', 'i', 'am'),\n",
       " ('i', 'am', 'looking')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams =  list(nltk.trigrams(wt_s))\n",
    "trigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "narrow-weather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'This', 'is', 'me', 'i'),\n",
       " ('This', 'is', 'me', 'i', 'am'),\n",
       " ('is', 'me', 'i', 'am', 'looking'),\n",
       " ('me', 'i', 'am', 'looking', 'for'),\n",
       " ('i', 'am', 'looking', 'for', 'you')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams =  list(nltk.ngrams(wt_s,5))\n",
    "ngrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-transformation",
   "metadata": {},
   "source": [
    "## Frequency Counter\n",
    "#### Counts Frequency of every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "daily-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "convertible-chess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 3), ('i', 3), ('you', 3), ('he', 3), ('am', 2)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in wt_s:\n",
    "    freq[i] = freq[i] + 1\n",
    "freq.most_common(5)           # get top 5 most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-functionality",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "banned-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  remove unusable word \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# print(\"Avilable languages stopwords : \\n\",stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "unsigned-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of words that are not important in NLP processes\n",
    "available_stopword = stopwords.words('english')\n",
    "\n",
    "# print(available_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "transparent-archive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after stopwords ['Hey', 'This', 'looking', 'since', 'past', 'year', 'days', 'brother', 'helping', 'steped', 'camping', 'seen', 'many', 'bridges', 'nothing', 'done', 'hounorable', 'rahul', 'honest', 'boy', 'america', 'newyork', 'state', 'elon', 'richest', 'among', 'owns', 'tesla', 'great', 'spacex', 'studied', 'precious', 'stanford', 'university', 'The', 'big', 'brown', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#  appending tokenzied words after removing stopwords\n",
    "\n",
    "\n",
    "tokens=[]\n",
    "\n",
    "for w in wt_s:  # wt_s  => tokenized words after removing punctuations and symbols\n",
    "    if w not in available_stopword:\n",
    "        tokens.append(w)\n",
    "\n",
    "#  word token after removing stopwords\n",
    "print(\"after stopwords\",tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-westminster",
   "metadata": {},
   "source": [
    "##     stremming\n",
    "### removing suffixes of base word , root word is known as stem and this is process to reach stem\n",
    "### Algorithms does'nt have knowledge of word it is processing \n",
    "####  e.g. help , helper , helped , helping are have same context so keep only base word help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "classified-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.porter import PorterStemmer  # stremming using porter stremmer algorithm\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "refined-stanford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stremmed \t Original Word\n",
      "hey \t == \t Hey\n",
      "thi \t == \t This\n",
      "look \t == \t looking\n",
      "sinc \t == \t since\n",
      "day \t == \t days\n",
      "help \t == \t helping\n",
      "stepe \t == \t steped\n",
      "camp \t == \t camping\n",
      "mani \t == \t many\n",
      "bridg \t == \t bridges\n",
      "noth \t == \t nothing\n",
      "hounor \t == \t hounorable\n",
      "ha \t == \t has\n",
      "own \t == \t owns\n",
      "studi \t == \t studied\n",
      "preciou \t == \t precious\n",
      "univers \t == \t university\n",
      "the \t == \t The\n"
     ]
    }
   ],
   "source": [
    "stremmed_tokes = []\n",
    "\n",
    "for w in wt_s:\n",
    "    stremmed_tokes.append(ps.stem(w))\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Stremmed \\t Original Word\")    \n",
    "for i in range(len(wt_s)):\n",
    "    if (stremmed_tokes[i] != wt_s[i] ):\n",
    "        print(stremmed_tokes[i] , \"\\t == \\t\",wt_s[i] )\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-entity",
   "metadata": {},
   "source": [
    "##   Lemmitizer\n",
    "### Algorithms refers dictionary before processing word that's causing high time and compute\n",
    "\n",
    "#### Diff. betn Stremming and lemitizing\n",
    "##### Stremming =>   studied  - > studi \n",
    "##### lemitizing =>   studied  - > study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "technological-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "included-graduate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized \t Original Word\n",
      "be  \t-->\t is\n",
      "be  \t-->\t am\n",
      "look  \t-->\t looking\n",
      "be  \t-->\t are\n",
      "be  \t-->\t am\n",
      "help  \t-->\t helping\n",
      "step  \t-->\t steped\n",
      "camp  \t-->\t camping\n",
      "see  \t-->\t seen\n",
      "bridge  \t-->\t bridges\n",
      "do  \t-->\t done\n",
      "be  \t-->\t is\n",
      "have  \t-->\t has\n",
      "be  \t-->\t is\n",
      "own  \t-->\t owns\n",
      "study  \t-->\t studied\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lemmitised_tokes=[]\n",
    "\n",
    "for w in wt_s:\n",
    "    lemmitised_tokes.append(wl.lemmatize(w,'v'))\n",
    "\n",
    "print(\"Lemmatized \\t Original Word\")     \n",
    "for i in range(len(wt_s)):\n",
    "    if (lemmitised_tokes[i] != wt_s[i] ):\n",
    "        print(lemmitised_tokes[i] ,\" \\t-->\\t\",wt_s[i])\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-carry",
   "metadata": {},
   "source": [
    "## POS Tagging [part of speech]\n",
    "### determining whether text is noun , verb or adjective \n",
    "### used for name, entity recognition or extracting word reletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "played-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "pos_tagged = nltk.pos_tag(wt_s)\n",
    "\n",
    "#  Get Nouns Only\n",
    "\n",
    "# nouns = [] \n",
    "# for i in pos_tagged:\n",
    "#     if i[1] == \"NN\":\n",
    "#         nouns.append(i[0])\n",
    "\n",
    "# print(\"Nouns are :\",nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "entertaining-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CC coordinating conjunction \n",
    "# CD cardinal digit \n",
    "# DT determiner \n",
    "# EX existential there (like: “there is” … think of it like “there exists”) \n",
    "# FW foreign word \n",
    "# IN preposition/subordinating conjunction \n",
    "# JJ adjective – ‘big’ \n",
    "# JJR adjective, comparative – ‘bigger’ \n",
    "# JJS adjective, superlative – ‘biggest’ \n",
    "# LS list marker 1) \n",
    "# MD modal – could, will \n",
    "# NN noun, singular ‘- desk’ \n",
    "# NNS noun plural – ‘desks’ \n",
    "# NNP proper noun, singular – ‘Harrison’ \n",
    "# NNPS proper noun, plural – ‘Americans’ \n",
    "# PDT predeterminer – ‘all the kids’ \n",
    "# POS possessive ending parent’s \n",
    "# PRP personal pronoun –  I, he, she \n",
    "# PRP$ possessive pronoun – my, his, hers \n",
    "# RB adverb – very, silently, \n",
    "# RBR adverb, comparative – better \n",
    "# RBS adverb, superlative – best \n",
    "# RP particle – give up \n",
    "# TO – to go ‘to’ the store. \n",
    "# UH interjection – errrrrrrrm \n",
    "# VB verb, base form – take \n",
    "# VBD verb, past tense – took \n",
    "# VBG verb, gerund/present participle – taking \n",
    "# VBN verb, past participle – taken \n",
    "# VBP verb, sing. present, non-3d – take \n",
    "# VBZ verb, 3rd person sing. present – takes \n",
    "# WDT wh-determiner – which \n",
    "# WP wh-pronoun – who, what \n",
    "# WP$ possessive wh-pronoun, eg- whose \n",
    "# WRB wh-abverb, eg- where, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ff19f",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "#### Chunking is a process of grouping words into a phrase to analyze the structure of the sentence\n",
    "#### Grouping contains POS tags as well as phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "acquired-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Name entity recognition\n",
    "\n",
    "# chuckes =nltk.ne_chunk(pos_tagged)   # only pos tagged input allowed\n",
    "\n",
    "# chuckes =nltk.ne_chunk(nltk.pos_tag([\"rahul\", \"pune\" , \"billion\",\"college\"]))   # only pos tagged input allowed\n",
    "\n",
    "# print(chuckes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "adjustable-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammer = r\" NAME: {RE}\"   # NAME can be anything releted to following rules\n",
    "\n",
    "\n",
    "grammer = r\" NP: {<DT|PP\\$>+<JJ>+<NN>}\"    \n",
    "        # determiner or progressive preposition at least once followed by adjectives at least once followed by noun only 1 time\n",
    "grammer = r\" NP: {<DT|PP\\$>+<JJ>+<NN>?}\"    \n",
    "        # determiner or progressive preposition at least once followed by adjectives at least once followed by noun optional or 1 time only\n",
    "grammer = r\" NP: {<JJ>+<NN>+}\"     # e.g. good elon , excellent college\n",
    "        #  adjectives at least once followed by noun at least once\n",
    "grammer = r\" Verb: {<PRP><VB|VBD|VBG|VBN|VBP|VBZ>+}\"     # e.g.i did , he took , she gave , we went\n",
    "        #  personal pronoun once followed by any type verb at least once\n",
    "grammer = r\"\"\" Verb: {<VB|VBD|VBG|VBN|VBP|VBZ>} \n",
    "                extract: {<PRP><Verb>}\"\"\"     # e.g.i did , he took , she gave , we went\n",
    "        #  using parse tree ,  verb will be extracted from personal pronoun once followed by any type verb at least once\n",
    "\n",
    "\n",
    "# ? => optional , * => if exist 0 or more times  , + => if exist atleast once\n",
    "# \\$ => progressive phrase word\n",
    "                                            \n",
    "# <DT>  => only one time determiner\n",
    "# <DT|NN> => only one determiner or noun \n",
    "                                            \n",
    "chunkparser = nltk.RegexpParser(grammer)\n",
    "\n",
    "tree = chunkparser.parse(pos_tagged)\n",
    "\n",
    "# print(tree)  # use draw instead cause print has limit to print lines\n",
    "\n",
    "tree.draw()  # Draws the tree in a window to visualize the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdebd04",
   "metadata": {},
   "source": [
    "## Chinking\n",
    "\n",
    "#### it is a process of removing specific chunks from the whole chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "headed-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer = \"\"\"\n",
    "            NAME: {RULES TO EXTRACT}\n",
    "            }RULES TO DONT EXTRACT FROM EXTRACTED{    \"\"\" # applied for all the included chunks\n",
    "  \n",
    "\n",
    "\n",
    "grammer = r\"\"\" NP: {<NN><.+>}       # including Noun followed by any type of word\n",
    "                }<NN><NN>{\"\"\"                 # out of selected chucks using including , exclude chunks that contains Noun followed by Noun \n",
    "                # e.g. selected => rahul did   but not    stanford university\n",
    "        \n",
    "        \n",
    "chunkparser = nltk.RegexpParser(grammer)\n",
    "\n",
    "tree = chunkparser.parse(pos_tagged)\n",
    "\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-tuesday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
