******************************************************************************************************************************************************************************************
										About Kubernetes
******************************************************************************************************************************************************************************************


* imperative means use cli every time to execute , declarative means make file and then execute it every time 

Cloud Native : architectural approach conatains workload that is poratable , modular and isolate between differnent cloud providers and cloud deployment models

k8s is based on cloud native

The Linux Foundation :
			formed in 2000's by merger between Open source development labs and Free student groups to support linux's growth and commerercial adoption 

CNCF (Cloud native Computing Foundation):
			formed in 2015 as a linux Foundation project to help advance container technology ,it operates independently from parent orgaization 


K8s , Prometheus , ETCD, ConaitnerD ,HELM  are some of the projects by CNCF

simply container management/orchestration tool , also known as k8s , Container orchestration automates the deployment, management, scaling, and networking of containers.
orchestration means clustering of any no of containers running on any network , it can works on also hybrid cloud also
Kubernetes was originally developed and designed by engineers at Google. it was developed in Golang , it supports menifest scripts in JSON and Yaml

container orchestration to automate and manage tasks such as:

	Provisioning and deployment
	Configuration and scheduling 
	Resource allocation
	Container availability 
	Scaling or removing containers based on balancing workloads across your infrastructure (can scale vertically[amount of resources] as well horizontally[no of containers])
	Load balancing and traffic routing 
	Monitoring container health
	Configuring applications based on the container in which they will run
	Keeping interactions between containers secure
	batch execution (manifest are used like recipe)

k8s is  Container as a Service (CaaS) . 
CaaS is cloud service that allows software developers and IT departments to upload, organize, run, scale, manage and stop containers by using container-based virtualization

Alternatives:
	AWS Fargate.
	Azure Container Instances. 
	Google Cloud Run.
	Google Kubernetes Engine (GKE) 
	Amazon Elastic Kubernetes Service (EKS) 
	Openshift Container Platform. 
	Rancher. 
	Docker Swarm.
	Apache marathon 


Kubernetes comparison with its top compatitor docker swarm :

	it has gui but swarm does not have
	it is complex and harder than swarm but it has more features than swarm
	data sharabilty is only within pod in case of kubernetes but in swarm it is sharable throughout all containers
	swarm only supports only while k8s supports  Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI 
	it has support of auto scaling in both direction but swarm does not have
	k8s has its own monitering tool but swarm needs 3rd party tool to perform this


Architecture :

	it has master- node architecture inside a cluter , there can be more than 1 master in this architecture
	, containers communicates with pods , diff pods can have diff types of container services running in it , 
	like in 1 pod 20 docker containers are running but in 2 nd pod from same cluster can have 30 OpenVZ or containerd conatiners running at same time 
	containers run microservice 


								  ------Cluster----------
								  |			 |
								master 		node --------------------------           (it can have multiple nodes for multiple purpoes)
											|                |            |
											pod1 	    	pod2    	pod3 		(generally all pods runs same application from same node)
											  | 		      |		    |		
											cont1		   cont1		cont1 		(diff types of container services can be used )
											cont2


	cluster -> nodes -> namesapce -> pods -> containers

	Kubernetes - Master Machine Components


	
			# Controller Manager
					This component is responsible for most of the collectors that regulates the state of cluster and performs a task.
					In general, it can be considered asa daemon which runs in nonterminating loop and 
							is responsible for collecting and sending information to API server. 

					It works toward getting theshared state of cluster and then make changes to bring the "current status" of
					  the server to "desired state".

					The controller manager runs different kind of controllers to handle nodes, endpoints, etc.
						like for every kind different controller is used

				


			# etcd
					It stores the configuration information which can be used by each of the nodes in the cluster. 
					It is a high availability key value store that can be distributed among multiple nodes. 
					It is accessible only by Kubernetes API server as it may have some sensitive information. 
					It is a distributed key value Store which is accessible to all.
					it is used to store cluster data so that in case if one of the node or even master fails it can be recovered.
					in EKS . every EKS cluster has its own etcd cluster , so it is not shared among nodes
					as etcd is open source , it is used by many other projects also like coreOs container , uber M3 app
			
			# Scheduler
					This is one of the key components of Kubernetes master. 
					It is a service in master responsible for distributing the workload. 
					It is responsible fortracking utilization of working load on cluster nodes and 
						then placing the workload on which resources are available and accept the workload.
					The scheduler is responsible for workload utilization and allocating pod to new node.

	
			# API Server
					Kubernetes is an API server which provides all the operation on cluster using the API. 
					API server implements an interface, which means different tools and libraries can readily communicate with it. 
					Kubeconfig is a package along with the server side tools that can be used for communication. 
					It exposes Kubernetes API.
					it is designed to scale horizontally that is by adding more servers/Instances
					multiple API servers can be used to provide balcaned traffic between instances
					can be accessed via  UI , API , CLI (kubectl)

A minimum Kubernetes master node configuration is:
	4 CPU cores (Intel VT-capable CPU)
	16GB RAM.
	1GB Ethernet NIC.
	40GB hard disk space in the /var directory.

On AWS
	sizes we use on AWS are(master requirements)

		1-5 nodes: m3.medium
		6-10 nodes: m3.large
		11-100 nodes: m3.xlarge
		101-250 nodes: m3.2xlarge
		251-500 nodes: c4.4xlarge
		more than 500 nodes: c4.8xlarge

				Master									Node


					it has all data about nodes 			 		  	sends data
						ETCD (can be accessed only by apiserver)   <-------------   Kubelet(each for every node)
					         					 ↑						 / --- Node 1  ---  pod11  -> cont1 , cont2
it checks etcd via server and commands           |						/
	 sheduler to perform actions		 		 |                     /				  
				CONTROL MANAGER	-------->   API SERVER ------------------------------ Node2  ----pod21  -> cont1 
						|							   |
						|							    -------- pod22  -> 	cont1 , cont2 , cont3			
						SCHEDULER (actions taken here actually)	
	

		* Admin sends menifest to Api server then 
		control manager check actual and desired state with help of etcd and 
		commands scheduler to perform necessory action to get in desired state , menifest scripts written in JSON OR Yaml contains
		scheduler samrtly decides which pod to run on which node and which container to run on which pod based on the workloads on each cluster and node
		but we can also decide manually to on which node pod will be created 
		by getting commnds from scheduler , kubelet on nodes perform actions to create pod based on image provided by scheduler itself
		Replication is done by Control manager by checking the desired and actual state of node and if any of pod fails ,new pod get scheduled on node
		etcd contains status of every node which is deliverd by kubelet from each nodes

		in case of 2 master structuer , api-server is getting load balanced and etcd is being shared by both master
	

	We can add additional master or worker node anytime in architecture , we can just to by installing required specification


	Kubernetes - Node Components


			# Kubelet Service (sends node status to etcd) 10225 port by default

					This is a small service in each node responsible for relaying information to and from control plane service. It interacts with etcd store to read 					
					configuration details and wright values. This communicates with the master component to receive commands and work. The kubelet process then assumes 					
					responsibility for maintaining the state of work and the node server. It manages network rules, port forwarding, etc.


			# Kubernetes Proxy Service (each pod has ip to communicate with other pods or external network )

					This is a proxy service which runs on each node and helps in making services available to the external host. It helps in forwarding the request to 					
					correct containers and is capable of performing primitive load balancing. 
					It makes sure that the networking environment is predictable and accessible and at the same time it is isolated as well. 
					It manages pods on node, volumes, secrets, creating new containers’ health checkup, etc.

		
			# Container Engine (e.g, Docker)

					The first requirement of each node is Docker which helps in running the encapsulated application containers in a relatively isolated but lightweight operating environment.




//////    Kubernaetes services 


					////  ***  kubectl    ***   ///
		
		The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. 
		You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs
		kubectl communicated with api-server to perform actions based on commands 


					////  ***  kind    ***   ///
		
		kind lets you run Kubernetes on your local computer. This tool requires that you have Docker installed and configured.

			
					////  ***   minikube   ***   ///

		Like kind, minikube is a tool that lets you run Kubernetes locally. 
		minikube runs a single-node Kubernetes cluster on your personal computer (including Windows, macOS and Linux PCs) so that you can try out Kubernetes, 
		or for daily development work.
		it has docker container time preinstalled so we dont need to install it but it is only available inside minikube 
		there will be different docker registry of docker inside minikube and locally installed docker

		use --vm-driver along with start to select hypervisor where minikube requires  , it is deprecated as  --driver
		--driver='': Driver is one of: virtualbox, vmwarefusion, kvm2, vmware,none, docker, podman, ssh (defaults to auto-detect)

		using : eval $(minikube docker-env)     , we can access docker inside minikube for that opened cli only 

				set imagePullPolicy to Never ,otherwise Kubernetes will try to download the image into its docker 

				to pull from local docker registry use :   minikube image load IMG_NAME            

				it will load the image into minikube docker registry from our local docker registry



					////  ***  kubeadm    ***   ///
			You can use the kubeadm tool to create and manage Kubernetes clusters. 
			It performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.


					////  ***  Kops    ***   ///

			Kops, short for Kubernetes Operations, is a set of tools for installing, operating, and deleting Kubernetes clusters in the cloud.
			A rolling upgrade of an older version of Kubernetes to a new version can also be performed. It also manages the cluster add-ons.


					////  ***  kublet    ***   ///

		kubelet: the component that runs on all of the machines/nodes in your cluster and does things like starting pods and containers.
		it is responsible for keeping the state of the node to desired state
		Also if user has to access container programatically , then kubelet is responsible for that


					////  ***  kubProxy    ***   ///

		it ensure that the pods are communicating with rights containers from other pods in same cluster 

		it also makes communication between service and pods

		This is a proxy service which runs on each node and helps in making services available to the external host. 
		
		It helps in forwarding the request to correct containers and is capable of performing primitive load balancing. 
		It makes sure that the networking environment is predictable and accessible and at the same time it is isolated as well. 
		
		It manages pods on node, volumes, secrets, creating new containers’ health checkup, etc.




HPA  (horizontal pod autoscale)

using resource metric (like cpu , memory) , we can do autoscaling of pods

in order to use HPA , we nedd to install metric server on cluster

here we set desired value and if the current value is more than desired value then it will scale up the pod

e.g. if current metric value is 200m and desired value is 100m then replicas will be doubled, since 200.0 / 100.0 == 2.0

in case of downscaling , we have cooldown period 
if the current value is less than desired value for cooldown period of time then it will scale down the pod





Tools can be used in Kubenetes :

	Logging : FluentD , FluentBit
	Monitering : DataDog , new Relic
	Cost : Kubecost , CloudHealth
	Storage : Portworx , Velero
	Security : Aqua , Twistlock
	Devops ; jenkins , gitlab


Kubernaetes On Cloud 

	AWS :
		ECS (Elastic Container Service)  => control plane costs zero , only pay for workernode ($70 per month per node)
											Requires creating cluster
 
		EKS(Elastic Kubernetes Service)	=> control plane costs $144/month , only pay for workernode 
											Requires creating cluster

		Farget     => no cluster required , 



Additional terms:

	In-Tree : plugins , components  or functions that are provided by default or resides within main repo
	Out-tree : plugins , components  or functions that must be installed manually