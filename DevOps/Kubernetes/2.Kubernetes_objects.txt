https://kubernetes.io/docs/
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands  
                             _              _                                    _                             
                            | | __  _   _  | |__     ___   _ __   _ __     ___  | |_    ___   ___ 
                            | |/ / | | | | | '_ \   / _ \ | '__| | '_ \   / _ \ | __|  / _ \ / __|
                            |   <  | |_| | | |_) | |  __/ | |    | | | | |  __/ | |_  |  __/ \__ \
                            |_|\_\  \__,_| |_.__/   \___| |_|    |_| |_|  \___|  \__|  \___| |___/
                                                                                                

                                    _         _                 _         
                             ___   | |__     (_)   ___    ___  | |_   ___ 
                            / _ \  | '_ \    | |  / _ \  / __| | __| / __|
                           | (_) | | |_) |   | | |  __/ | (__  | |_  \__ \
                            \___/  |_.__/   _/ |  \___|  \___|  \__| |___/
                                            |__/                           



kubernetes offers following objects/components :


diff types of object available in k8s are :   (around 50+ Resources/objects available in k8s )


					---------------------------------------------------------------------------
					|                          Pod					                        |
					---------------------------------------------------------------------------

                    it is smalles unit of k8s, it is used to run application on k8s cluster.
                    A pod is a collection of containers and its storage inside a node of a Kubernetes cluster. 
                    It is possible to create a pod with multiple containers inside it. 


                    pod has its own ip address but containers inside pods not have their own ip address

                    evrytime pod gets created it will get new ip address only , dns name of every container of every pod is stored at ( /etc/resolv.conf   of any linux)

                    if a single container inside pod fails then whole pod is discarded and new pod is creared as that pod cant be repaired or reconstructed, 
                    this is disadvatage of multi conatainer pod .
                    so , it is recommended that to use Single container

                    we can get in to pod also with using exec 

                    if pod having resource quota that has limit exceeding limits defined in limit range / service quota , pod will not created
                    same with request , if it is below the mentioned it will not created

                    Pod is running and has two Containers. Container 1 exits with failure.

                        1.Log failure event.

                        2.If restartPolicy is:

                            Always: Restart Container; Pod phase stays Running.

                            OnFailure: Restart Container; Pod phase stays Running.

                            Never: Do not restart Container; Pod phase stays Running
                                So pod is not restarted or the other container is not restarted , 
                                only the exited container is restarted based on the restartPolicy
                    

                    probe is used to detect state of container , if it is not responding then pod is restarted

                    Liveness Probe :   kubelet uses this when to restart container
                            usecase :  liveness probes could catch a deadlock, where an application is running, but unable to make progress. 
                                        Restarting a container in such a state can help to make the application more available despite bugs.

                    Readiness Probe : kubelet uses readiness probes to know when a container is ready to start accepting traffic. 
                            usecase :  readiness probes could be used to detect a container that is not ready to accept traffic. 
                                       When a Pod is not ready, it is removed from Service load balancers.
                    
                    Startup Probe : kubelet uses startup probes to know when a container application has started. 
                                    If such a probe is configured, it disables liveness and readiness checks until it succeeds, 
                                            making sure those probes don't interfere with the application startup.
                                    avoids pods getting killed by the kubelet before they are up and running



                    --------------------   Init Containers  ---------------------------------

                    Init containers can contain utilities or setup scripts not present in an app image.

                    Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started.

                    If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.
                    if init succeeds then only app container will be started

                    To specify an init container for a Pod, add the initContainers field into the Pod specification, 
                        as an array of container items (similar to the app containers field and its contents). 

                    init containers do not support lifecycle, livenessProbe, readinessProbe, or startupProbe because they must run to completion before the Pod can be ready.

                    specifying multiple init containers for a Pod, kubelet will runs each init container sequentially . 
                    Each init container must succeed before the next can run.


                    ------------------------ Pod lifecycle ---------------------------------

                    Phases of a Pod =:

                    Pending	=> Pod has been accepted by the Kubernetes cluster, but is waiting for resources to be allocated.
                                This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.

                    Running	=>  Pod has been bound to a node, and all of the containers have been created. 
                                At least one container   is still running, or is in the process of starting or restarting.

                    Succeeded =>  All containers in the Pod have terminated in success, and will not be restarted.	

                    Failed	 =>  All containers in the Pod have terminated, and at least one container has terminated in failure

                    Unknown	 =>   For some reason the state of the Pod could not be obtained. 
                                    This phase typically occurs due to an error in communicating with the node 
                                    where the Pod should be running.

                    Terminating status is not one of the Pod phases. A Pod is granted a term to terminate gracefully, which defaults to 30 seconds.

                    init:N/M means the Pod has M Init Containers, and N have completed so far.

                    ------------------------ Pod Conditions ---------------------------------

                    all of the following has boolean value true or false , we can get this using describe

                    PodScheduled =: the Pod has been scheduled to a node.

                    ContainersReady =: all containers in the Pod are ready.

                    Initialized =: all init containers have completed successfully.

                    Ready =: the Pod is able to serve requests and 
                                should be added to the load balancing pools of all matching Services.

					---------------------------------------------------------------------------
					|                                    Services				         	 |
					---------------------------------------------------------------------------

                    A “service” is defined as the combination of a group of pods, and a policy to access them
                    A service needs three things: 
                            a name (mychatapp-service), 
                            a way to identify the pods in its group (typically a label like irc=forever),  
                            way to access those pods (port 6667 via TCP)
                    
                    services are attached to pods , even if pod dies , services remain as it is and can be attched to new pod 
                        it is done by using static ip and DNS name for set of pods

                    each service has its own ip address and port number

                    service creates static ip and then uses iptable which are installed on node to NAT and Load balancing to other pods

                    | --------------  POD1 ----------------|

                    pod1-endpoint  - NNS1 - eth0 >-  v-eth 0 \
                                                     --- LB -- ipTable -- eth0 - > rootNNS         (network namesapce)
                    pod2-endpoint  - NNS2 - eth0  >- v-eth 1 /

                    so that they are used in crusial container application like database so that even the db pod get terminated , 
                    data and connection to db by other services will never get lost

                    there are 2 types traffic policies of services available in k8s:

                            1. Internal traffic policies : 

                                        used in pods that not need to expose to world like database , only require to be used by other pods

                            2. External traffic policies :

                                        used in pods that need to be exposed to world like web server 

                    there are 3 types available in service :

                        1. ClusterIP   :    Default , accessible only within cluster (cluster can contains multiple nodes)
                                            randomly forward traffic to any node pod set with target port

                        2. NodePort    :     can be accessible static port on each node , 
                                                this static port in defined in .spec.ports.nodePort
                                                static port should be between 30000 - 32767 , other than that not accepted
                                                not secure and not efficient , used only for test
                                                it make the ClusterIP service available from the outside
                                                It is an open port on every worker node in the cluster that has a pod for that service
                                                Exposes the Service on each Node's IP at a static port (the NodePort). 
                                                A ClusterIP Service, to which the NodePort Service routes, is automatically created. 
                                                You'll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort>.


                        3. LoadBalancer :    It supports multiple protocols and multiple ports per service
                                            Exposes the Service externally using a cloud provider's load balancer. 
                                            NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.

                                            external load balancer handles routing and traffic distribution logic


                    ClusterIP < Nodeport < LoadBalancer 

                        ExternalName : it creates an internal service with an endpoint pointing to a DNS name. e.g. aws RDS

                                same as clutser ip but instead of returning static ip address it returns CNAME record

					---------------------------------------------------------------------------
					|                                 Headless Service  	 			 |   (used for statefulset) 
					---------------------------------------------------------------------------

                    Headless Service is a service that does not have a corresponding LoadBalancer and static IP
                    Setting the clusterIP field in a service spec to None makes the service headless

                    Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP — the service’s cluster IP. 
                    But if you tell Kubernetes you don’t need a cluster IP for your service, the DNS server will return the pod IPs instead of the single service IP

                    A Headless service is used to identify specific pods by assigning them DNS record

                    Sometimes you don't need load-balancing and a single Service IP. 
                    In this case, you can create what are termed "headless" Services, by explicitly specifying "None" for the cluster IP (.spec.clusterIP).

                    headless services still provide load balancing across pods but through the DNS round-robin mechanism instead of through the service proxy.


					---------------------------------------------------------------------------
					|                                    Endpoints				         	 |
					---------------------------------------------------------------------------

                    it is way to link service to pod or set of pods

                    An endpoint is an resource that gets IP addresses of one or more pods dynamically assigned to it, along with a port.

                    An endpoint resource is referenced by a kubernetes service, 
                        so that the service has a record of the internal IPs of pods in order to be able to communicate with them.

                    We need endpoints as an abstraction layer because the 'service' in kubernetes acts as part of the orchestration to ensure distribution of 
                        traffic to pods (including only sending traffic to healthy pods). 

                    For example if a pod dies, a replacement pod will be generated, with a new IP address. 
                    Conceptually, the dead pod IP will be removed from the endpoint object, and the IP of the newly created pod will be added, 
                        so that the service is updated and 'knows' which pods to connect to.

                        Service  --  Endpoint1  ------  pod1
                                |      | 
                                |      |------------ pod2
                                |
                                |--  Endpoint2   ---------- pod3

                    
                    If you do not specify a label selector in your service - 
                        Kubernetes can’t create the list of endpoints because he doesn’t know which pods should be included and proxied by the service.


			        ---------------------------------------------------------------------------
					|                      StatefulSets     			             	 |
					---------------------------------------------------------------------------

                    there will be data inconsistancy if 2 or more pods performing same operation at same/different time cauing data loss/error 
                    like transaction control in database

                    so that statefulsets are used in stateful operations like database 

                    because of that stateful sets are used to manage stateful pods not the deployments

                    it also manages replication of pods just like deployment

                    it sures that read and write in this pods are synchronized

                    it is difficult than deployments 

                    Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. 
                    These pods are created from the same spec, but are not interchangeable: 
                                    each has a persistent identifier that it maintains across any rescheduling.

					---------------------------------------------------------------------------
					|                            DaemonSets				            	 |
					---------------------------------------------------------------------------

                    A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. 
                    As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.

                    Daemonset is another controller that manages pods like Deployments, ReplicaSets, and StatefulSets. 
                    It was created for one particular purpose: ensuring that the pods it manages to run on all the cluster nodes. 
                    As soon as a node joins the cluster, the DaemonSet ensures that it has the necessary pods running on it. 
                    When the node leaves the cluster, those pods are garbage collected.

                    By default, a DaemonSet schedules its pods on all the cluster nodes

                    Also DaemonSets allow you to select which nodes you want to run the pods on. 
                    With nodeSelector, you can select nodes by their labels

                    it is not possible in AWS EKS so we need to use sidecar instead of DaemonSets

                 

               		---------------------------------------------------------------------------
					|                                         Ingress 	        				 |  (internet to k8s pod)
					---------------------------------------------------------------------------


                    it is type of External service that is used to expose services to the world but using a secure protocol like https and with domain name

                    traffic routing is controlled by rules defined on ingress resource
                    to use ingress , we need ingress controller 

                    if request comes to the domain name , it will be routed to the service by ingress

                    it is used in real world applications not the LoadBalancer


                    ---------------------------------------------------------------------------
					|                           Volume      				 				 |
					---------------------------------------------------------------------------

                    Volume is a container for a file or directory on the host machine that is mounted into a container. it is user managed
                    it can be on cloud also. like awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS volume into your pod
                    concept of volume was present with the Docker, however the only issue was that the volume was very much limited to a particular pod.
                     As soon as the life of a pod ended, the volume was also lost.


                     Pod can use any number of volume types simultaneously. 
                     Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyond the lifetime of a pod. 
                     When a pod ceases to exist, Kubernetes destroys ephemeral volumes; however, Kubernetes does not destroy persistent volumes. 
                     For any kind of volume in a given pod, data is preserved across container restarts.


                    Types of Volumes in Kubernetes :


                            Persistent Volumes  :  A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by 
                                                        an administrator or dynamically provisioned using Storage Classes. 
                                                   PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. 
                                                   it is similar to node in cluster , it does not reside in within node or pod

                                                   name of a PersistentVolume object must be a valid DNS subdomain name.

                                            e.g. hostpath  , here both pod and host can write and read data 
                                                        hostPath volume mounts a file or directory from the host node's filesystem into your Pod

                                            apiVersion: v1
                                            kind: Pod
                                            metadata:
                                            name: myvolhostpath
                                            spec:
                                            containers:
                                            - image: centos
                                                name: testc
                                                command: ["/bin/bash", "-c", "sleep 15000"]
                                                volumeMounts:
                                                - mountPath: /tmp/hostpath
                                                name: testvolume
                                            volumes:
                                            - name: testvolume
                                                hostPath:
                                                path: /tmp/data   # at this location of host machine 

                                            mounting PV directly to pod is not allowed as it is aganinst k8s design priciple that describes tight coupling between 2 resources not allowed
                                            it could caude tight coupling below the pod and underlying storage 
                                            
                                            so that PVC is used instead of PV to ensure decoupling

                            Projected Volumes   :  A projected volume maps several existing volume sources into the same directory.
                                                    Currently, the following types of volume sources can be projected:
                                                                secret
                                                                downwardAPI
                                                                configMap
                                                                serviceAccountToken


                            Ephemeral Volumes   :  volumes follow the Pod's lifetime and get created and deleted along with the Pod

                                        e.g.  = emptydir  , here in a single pod we can share same volume among different containers

                                          - name: c1
                                            image: centos
                                            command: ["/bin/bash", "-c", "sleep 15000"]
                                            volumeMounts:                                    
                                            - name: xchange
                                                mountPath: "/tmp/xchange"               # avilable in this path 
                                         - name: c2
                                            image: centos
                                            command: ["/bin/bash", "-c", "sleep 10000"]
                                            volumeMounts:
                                            - name: xchange
                                                mountPath: "/tmp/data"
                                         - name: xchange
                                            emptyDir: {}
                            
                            Volume snapshots :  Archiving volume configuration and its data for rollbacks or backups

                            other types of volume supported :
                                secrets
                                configmap
                                nfs
                                EBS
                                PersistentVolumeClaim


					---------------------------------------------------------------------------
					|                                     Storage Class		 			 |
					---------------------------------------------------------------------------


                    it is way of defining class of storage that is backed by provider/provisioner like AWS , GCP , Azure etc.
                    Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators

                    Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy, 
                            which are used when a PersistentVolume belonging to the class needs to be dynamically provisioned.

                    PersistentVolumes that are dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of the class, 
                            which can be either Delete or Retain. If no reclaimPolicy is specified when a StorageClass object is created, it will default to Delete.

                    PersistentVolumes that are created manually and managed via a StorageClass will have whatever reclaim policy they were assigned at creation.

                        e.g.  for aws ebs

                                        apiVersion: storage.k8s.io/v1
                                        kind: StorageClass
                                        metadata:
                                            name: standard
                                        provisioner: kubernetes.io/aws-ebs    # whose storage
                                        parameters:                             # what type of storage
                                            type: gp2
                                        reclaimPolicy: Retain            # retain or delete storage after pod goes down , delete is default
                                        allowVolumeExpansion: true    # Directly editing the size of a PV can prevent an automatic resize of that volume as master assumes no resize is necessary
                                        mountOptions:
                                            - debug
                                        volumeBindingMode: Immediate


					---------------------------------------------------------------------------
					|                  Persistent Volume Claim		(PVC)	 	 |
					---------------------------------------------------------------------------


                    it is used to decouple PV from pods . it just allows pods to access storage class using PV.

                                    pod -> PVC ------ bind  ----> PV -------> storage class

                    PVC asks for type of storage and if a PV matches criteria , that PV will be claimed and bound .

                    it should be in same namespace as a pod is .  

                    PVC can request for specific size of storage and access mode like ReadOnlyOnce or ReadWrite

                    PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. 
                    The interaction between PVs and PVCs follows this lifecycle:

                        Provisioning :
                                        Static
                                            A cluster administrator creates a number of PVs
                                        Dynamic
                                            When none of the static PVs the administrator created match a user's PersistentVolumeClaim, 
                                            the cluster may try to dynamically provision a volume specially for the PVC.

                        Binding  :  
                                    A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. 
                                    If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. 
                                    Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. 
                                    Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. 

                        Using
                                    Pods use claims as volumes. 
                                    The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. 
                                    For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.

                                    Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. 
                                    Users schedule Pods and access their claimed PVs by including a persistentVolumeClaim section in a Pod's volumes

                            If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. 
                            PVC removal is postponed until the PVC is no longer actively used by any Pods. 
                            Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC.

                            You can see that a PVC is protected when the PVC's status is Terminating and the Finalizers

                        Reclaiming

                                    When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. 
                                    The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. 
                                    Currently, volumes can either be Retained, Recycled, or Deleted

                                    Retain
                                            The Retain reclaim policy allows for manual reclamation of the resource. 
                                            When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered "released". 
                                            But it is not yet available for another claim because the previous claimant's data remains on the volume

                                    Delete
                                            For volume plugins that support the Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, 
                                                as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume

           			 ---------------------------------------------------------------------------
					|                               configmap               				 |
					---------------------------------------------------------------------------

                    In programming, we use env files or separate configuration files to store settings, configurations, or variables that are required to execute the program. 
                    In Kubernetes, we can use ConfigMaps to achieve the same functionality.
		            A ConfigMap is a Kubernetes API object that can be used to store data as key-value pairs. Kubernetes pods can use the created ConfigMaps as a:

                                    Configuration file
                                    Environment variable
                                    Command-line argument

		            ConfigMaps provides the ability to make applications portable by decoupling environment-specific configurations from the containers.

                    * Importantly, ConfigMaps are not suitable for storing confidential data. They do not provide any kind of encryption, and all the data in them are
                            visible to anyone who has access to the file. (Kubernetes provides secrets that can be used to store sensitive information.)





                    ---------------------------------------------------------------------------
					|                                 secrets			        	   	 |
					---------------------------------------------------------------------------

                    Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. 
                    Such information might otherwise be put in a Pod specification or in a container image. 
                    Using a Secret means that you don't need to include confidential data in your application code.

                    Because Secrets can be created independently of the Pods that use them, 
                    there is less risk of the Secret (and its data) being exposed during the workflow of creating, 
                    viewing, and editing Pods. Kubernetes, and applications that run in your cluster, can also take additional precautions with Secrets, 
                    such as avoiding writing confidential data to nonvolatile storage.

                    Secrets are similar to ConfigMaps but are specifically intended to hold confidential data.
                    it is stored in etcd with base64 encoding , now we can encrypt it using KMS 
                        so that only using application specific keys we can access the data 

                    by default , secrets are unencrypted in etcd store






			    	---------------------------------------------------------------------------
					|                            ReplicationController		     			 |
					---------------------------------------------------------------------------

                    Replication Controller is one of the key features of Kubernetes, which is responsible for managing the pod lifecycle. 
                    It is responsible for making sure that the specified number of pod replicas are running at any point of time. 
                    It is used in time when one wants to make sure that the specified number of pod or at least one pod is running. 
                    It has the capability to bring up or down the specified no of pod.



                   	--------------------------------------------------------------------------
					|                                    ReplicaSet  						 |
					---------------------------------------------------------------------------
                      --- advanced version of ReplicationController , it support equality as well as set based selectors

                                A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. 
                                As such, it is often used to guarantee the availability of a specified number of identical Pods.


					---------------------------------------------------------------------------
					|                         Deployment   			                		 |
					---------------------------------------------------------------------------


                    more powerful than replicaset  , it can manage multiple replicaset
	                we can do updates and rollbacks using deployment but it is not possible in case of replicaset
					it deploys pods using replicaset only
                    it is abstraction on replica set that is abstraction on pod that is abstraction on container

                    A Deployment provides declarative updates for Pods and ReplicaSets.
                    it changes actual state to desired state at controlled rate

                    Naming in deployment :

                            Deployment : mydeploy

                            replicaset(rs) : mydeploy-rs_id

                            pods : mydeploy-rs_id-pod_id


                    Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.

                    Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. 
                    A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. 
                    Each new ReplicaSet updates the revision of the Deployment.

                    Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.

                    Scale up the Deployment to facilitate more load.

                    Pause the rollout of a Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.

                    Use the status of the Deployment as an indicator that a rollout has stuck.

                    Clean up older ReplicaSets that you don't need anymore.

                    Everytime when deploy named <deployment-name> is updated or runned , it will create version for that and using rollout we can rollback 
                    to any version of that deployment



                    In order to rollout back to previous specific version we use :

                            kubectl rollout status deployment <deployment-name>

                            kubctl rollout undo deployment <deployment-name> --to-revision=<revision-number>          -- get rivision number using history commnd

                    To see the rollout history of a deployment we use :

                            kubectl rollout history deployment <deployment-name>


                    To change scale of deployments use :

                            kubectl scale --replicas=<number-of-replicas> deployment <deployment-name> 


                    After changing some of the deployment parameters and after applying again will lead to create new replicaset 
                    previous replicaset will remain as it is , only pods inside that will be terminated 
                    new pods will be created inside the new replicaset and no. of pods will be equal to number of replicas specified in deployment


                    Deploy ment may fails due to Following reasons :

                    1. insufficient quota on nodes to deploy new pods
                    2. Readiness probe failure   -> occures when node is not ready to serve new pods 
                    3. Image pull Error : mentoined container image is not available or it is not in local registry and imagePullPolicy is set to never
                    4. insufficient Permissions
                    5. Limit Ranges : when node reaches its limit 


                    stratergies for deployment : (mentioned in manifest at spec.stratergy.type)

                        1. Recreate  => when all old pods is terminated , it will be created with new once , downtime can happen , rollback not possible to old exact infra
    default strategy    2. RollingUpdate => replace one or more pods with new one ,  , no downtime until bug in new enviroment , slow 
                        3. canary  => add new pod and route some traffic , if no bugs occures , terminate old enviroment and shift to new one
                        4. blue / green   => deploy new env(green) on entire infrastructure , swap traffic and if no bugs then terminate old enviroment(blue) and shift to new one
                        5. A/B testing => similar to A/B but instead of swap all users it splits traffic between two enviroments 
                        

                        only Recreate and RollingUpdate are supported by deployment

		


					---------------------------------------------------------------------------
					|                               Namespace		    	            	 |
					---------------------------------------------------------------------------

                    namespaces provides a mechanism for isolating groups of resources within a single cluster. 
                    Names of resources need to be unique within a namespace, but not across namespaces. 
                    Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and 
                    not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc)

                    K8s starts with 4 initial namespaces:
 
                    1. default    :   By default namespace 
                    2. kube-system    :  system namespace that stores objects created by kubernetes system  , do not use this namespace for your own resources
                    3. kube-public    : For resources that are publically available
                    4. kube-node-lease   :  namespace for node lease objects , used to detect node failure by sending heartbeat to kube-apiserver

                    Single NS Objects : ConfigMaps and Secreats , cant be shared across namespaces limited to only one
                    multiple NS Objects : pods and services cam belong to more than one NS
                    Cluster-Wide :   Vloumes and Nodes are global objects , cant be bound to a namespace

                    using tool kubens we change default namesapce to any names space we want

                    Resource Quoto is allocated to namespace , all the objects created in that namespace can not use exceed resource qutoa limit



                    By default resource quota is set to 0 means no limit to access resources

                    we can define resurce qutoa on   1. Ram   2. storage    3. CPU

                    here we have 2 options :
                            1. Request  => amout of resources object needed , , if not mentioned by default it is same as limit

                            2. Limit  => maximum resources object can use , if not mentioned by default it is maximum available
                    
                    we can use resurce quota imperatively as well as declaratively

                    we can use resource quota declaratively by using :

                            kubectl create quota <quota-name> --hard=<resource-name>=<value>

                            memory refers to RAM

                                    resource-name can be = 

                                          limits.<Resource-Object>=Value      e.g.  limits.cpu: "400m"

                                          requests.<Resource-Object>=Value      e.g.  requests.memory: "200Mi"
  
                                only   cpu considered	Same as requests.cpu
                    
                    The resource quota is the total available resources for a particular namespace, while limit range is used to assign limits for containers(Pods) running inside the namespace.
                    ResourceQuota is for limiting the total resource consumption of a namespace
                    LimitRangeis for managing constraints at a pod and container level within the project.

				Namespace provides an additional qualification to a resource name. This is helpful when multiple teams are using the same cluster and there is a potential of 			
                	name collision. It can be as a virtual wall between multiple clusters.

				Functionality of Namespace
					Following are some of the important functionalities of a Namespace in Kubernetes −

												Namespaces help pod-to-pod communication using the same namespace.

												Namespaces are virtual clusters that can sit on top of the same physical cluster.

												They provide logical separation between the teams and their environments.
	
					---------------------------------------------------------------------------
					|                            ResourceQuota                  	 	 |
					---------------------------------------------------------------------------

                    we can define ResourceQuota using declarative method

                    The name of a ResourceQuota object must be a valid DNS subdomain name.

                    If creating or updating a resource violates a quota constraint, the request will fail with 
                    HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated.

                    it will applied to current namespace only , every object created in that namespace will be checked against quota

                    with resource quota we can define max pods will be created inside a namesapce

					---------------------------------------------------------------------------
					|                             Limit Range	                     	 |
					---------------------------------------------------------------------------

                    it used to set limit on object's max resource use

                    Within a namespace, a Pod or Container can consume as much CPU and memory as defined by the namespace's resource quota.

                    There is a concern that one Pod or Container could monopolize all available resources.
                     
                    A LimitRange is a policy to constrain resource allocations (to Pods or Containers) in a namespace.

                    LimitRange provides to Enforce minimum and maximum compute resources usage per Pod n a namespace.
                    Also Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.

                    name of a LimitRange object must be a valid DNS subdomain name

                    LimitRange validations occurs only at Pod Admission stage, not on Running Pods.

                    If any container in that Pod does not specify its own CPU request and limit, the control plane assigns the default CPU request and limit to that container. 
                            i.e. which is mentioned in LimitRange Object

                    When creating a LimitRange object, you can specify limits on huge-pages or GPUs as well. 
                    However, when both default and defaultRequest are specified on these resources, the two values must be the same.

                    



	
             
	
             
					---------------------------------------------------------------------------
					|                              Network policy  	        	 			 |
					---------------------------------------------------------------------------

                    it is used to restrict network access to a pod or whole namespace

                    with this , we can set policies so that only allowed namespaces traffic can come

                    selectors are used resources with matching lables for network policy to be applied on 

                    to use this , we need network policy agents like Antrea 
             
	
					---------------------------------------------------------------------------
					|                                Liveness		 	        		 |
					---------------------------------------------------------------------------

                    Using liveness we can check if the service inside pod is running properly or not.
                    

					---------------------------------------------------------------------------
					|                                   Job     		 	        		 |
					---------------------------------------------------------------------------

                    performs batch operations

                    like for log opearations we need to terminate pod after successful completion of log operations , Backup database , etc
                    Kubernetes Jobs ensure that one or more pods execute their commands and exit successfully.

                    When all the pods have exited without errors, the Job gets completed. When the Job gets deleted, any created pods get deleted as well.

                    Pods gets deleted after successful ecxecution of job , but job remains , we need to delete manually 

                    lifecycle:
                        1. job is created
                        2. pods are created according to job spec
                        3. containers inside pod perform operation that are mentioned.
                        4. when all containers are done , pod gets terminated, job is completed

                    
                    CRON Jobs :

                    cron job is a type of job that runs periodically.


					---------------------------------------------------------------------------
					|              RBAC(role based access control) API               		 |
					---------------------------------------------------------------------------

                    Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization.

                    uses the rbac.authorization.k8s.io API group to drive authorization decisions, allowing you to dynamically configure policies through the Kubernetes API.

                    has 4 kinds of objects : 1. Role 2. RoleBinding 3. ClusterRole 4. ClusterRoleBinding

                    Role is set of permissions for a namesapce , when you create a Role, you have to specify the namespace it belongs in.

                    cluster Role is set of permissions across all namespaces , it  is a non-namespaced resource

                    binding means linkng permission to subject/identity

                    role binding grants the permissions defined in a role to a user or set of users. 
                    It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted.

                 
     
    DeploymentController (Manages Pods)
	
	  ----- 

You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate.
 You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.


Other Objects ;

            Custom Resource Definition (CRD) : used for custom controller

            Binding

            Componentstatuses

            

            events 

            LimitRange

            Podtemplates 

            Resourcequota 

            serviceaccounts

            apiservices

            Tokenreviews

            certificate signing request

            Leases 

            
            Roles 
            





    
    
