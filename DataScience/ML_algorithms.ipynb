{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Machine Learing Algorithm \n",
    "\n",
    "### contents :\n",
    "\n",
    "####  * Basics , Problem Faced and Errors*\n",
    "#### 1. Supervised Learning\n",
    "##### 1.1 Regression => Linear Regression , SVM , Decision Tree , Random Forest , Ridge and Lasso , Bayesian Ridge , OLS , KNN\n",
    "##### 1.2 Classification => Logistic Regression , KNN , SVM , Decision Tree , Random Forest , naive Bayes , Decision Trees,Naïve Bayes\n",
    "##### 1.3 Ensemble Techniques => Bagging , AdaBoost , Gradient Boosting , XGBoost\n",
    "#### 2. Unsupervised Learning\n",
    "##### 2.1 Clustering  => K-Means , DBSCAN , Agglomerative Clustering , Spectral Clustering\n",
    "##### 2.2 Dimensionality Reduction => PCA , LDA , MDS , Isomap , t-SNE\n",
    "#### 3. Semi - Supervised  Learning\n",
    "#### 4. Reinforcement Learning  => Q-Learning , DQN , A2C , PPO , DDPG , TD3\n",
    "________________________________________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Faced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bias: Assumptions made by a model to make a function easier to learn , It is the difference between the average prediction and the target value.  it is Error of training Data\n",
    "##### Variance: If you train your data on training data and obtain a very low error, upon changing the data and then training the same previous model you experience a high error, this is variance.  it is Error of testing Data\n",
    "##### <i> bias is releted to training data and varience is releted to testing data </i>\n",
    "\n",
    "#### <b> Overfitting:  </b>\n",
    "##### The scenario when a machine learning model is able to perform well on a training set, but not on a test set.\n",
    "##### model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set.\n",
    "##### when model perform well on training data known as low bias\n",
    "##### when model fails on testing data , this condition is known as high variance\n",
    "##### Overfitting model satisfies both conditions : high variance and low bias\n",
    "##### Generally, Decision trees are prone to Overfitting.\n",
    "##### e.g. traing accuracy is 92% but testing accuracy is around 70 - 80 %.\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/83094acc.png\"  height=\"40%\" width=\"50%\">\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/74152loss.png\"  height=\"40%\" width=\"50%\">\n",
    "\n",
    "#### What To Do :\n",
    "##### 1. Reduce the number of features\n",
    "##### 2. Reduce model complexity.\n",
    "##### 3. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "##### 4. Ridge Regularization(L2) and Lasso Regularization\n",
    "##### 5. Use dropout for neural networks to tackle overfitting.\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/20790High%20Bias.gif\"  height=\"40%\" width=\"50%\">\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171258/overfitting_2.png\"  height=\"70%\" width=\"70%\">\n",
    "\n",
    "#### <b> Underfitting:  </b>\n",
    "##### The scenario when a machine learning model is not able to perform well on a training set also as well as test set.\n",
    "##### it cannot capture the underlying trend of the data. usually happens when we have fewer data to build an accurate model\n",
    "##### Underfitting model satisfies both conditions : High Bias and Low Variance\n",
    "##### Generally, Linear and Logistic regressions are prone to Underfitting.\n",
    "##### e.g. traing accuracy is 60 -70 % and testing accuracy is around 50 - 60 %.\n",
    "#### What To Do :\n",
    "##### 1. Increase the number of features\n",
    "##### 2. Remove noise from the data.\n",
    "##### 3. Increase the number of epochs or increase the duration of training to get better results.\n",
    "\n",
    "## Optimal Model\n",
    "### Low Bias And Low variance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## 1. Supervised Learning\n",
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output.\n",
    "#### Y = f(X)\n",
    "##### There are 2 types of supervised learning algorithms:\n",
    "##### 1.1 Regression\n",
    "##### 1.2 Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### ____________________________ 1.1 Regression _____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Also known as estimation model\n",
    "#### regression is the process of predicting a continuous value\n",
    "##### Performance Metrics is used in case of regression\n",
    "##### if gradient descent is invilved in model then use Standerd scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ****      Regression Algorithms         ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ---------- <b> 1.1.1 Linear Regression  </b> ------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear regression is a simple linear model that assumes that the relationship between the input variables and the output variable is linear.\n",
    "##### here we try to find the best fit line for the data , here y is linear function of x , we predict y based on values of x and parameters of the line\n",
    "##### here we use equation of the line to predict y i.e. y = mx + c   _OR_  y = (Mi)(Xi) + C   where i is the index of the input variable\n",
    "##### it is writen in standerd terms as <h1> h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x      </h1>\n",
    "#####  where m is the slope(unit movement in x axis then respective movement in y axis) of the line and c is the intercept (at what point does the line intersect the y-axis)\n",
    "##### here we try to minimize the distance between the data points and the line that is known best-fit line\n",
    "##### here we calculate the cost function\n",
    "### <h1>J(<b>θ</b><sub>0</sub>,<b>θ</b><sub>1</sub>) = 1/2n *  Σ(h(x<sub>i</sub>) - y<sub>i</sub>)²   </h1> &nbsp;  <sub> theta_0 = C , theta_1 = M , h(x<sub>i</sub>) = predicted/calculated value ,  y<sub>i</sub> = Actual value </sub>\n",
    "#####  it is required to update values of slope and c in order to minimize the error  , summation is considerd from i=1 to i=n , also  h(xi) is predicted value of y for xi and yi is actual value of y for xi  --- we divided here by 1/2n because by 1/n we get avarage of error  and 1/2 for differentiate helper\n",
    "#####   according to the cost function we can plot the graph of cost function and find the minimum value of cost function , it is also called gradient descent algorithm , here we take random value once of slope and c and find update it and finds the minimum value of cost function  \n",
    "\n",
    "##### to update the slope and c we use the formula known as convergence algorithm(Repeat until convergence)\n",
    "### Repeat until convergence {\n",
    "#####   <b>θ</b><sub>j</sub> = <b>θ</b><sub>j</sub> - α * δ/δ<sub><b>θ</b><sub>j</sub></sub> J( <b>θ</b><sub>0</sub>, <b>θ</b><sub>1</sub>)  \n",
    "### }\n",
    "#####    α is learning rate , δ is partial derivative of cost function with respect to theta 0 and theta 1\n",
    "##### If we choose our learning rate too large, then there is a problem of overshooting and not achieving our desired convergence point(global minimum point). \n",
    "##### II) At, the same time if we choose our learning rate to be too small, then it may take a lot of time to get to the convergence point, which is not very time consuming, but we still could reach to our global minimum.\n",
    "<img src=\"https://miro.medium.com/max/1400/1*o95nDGY2oV9r3jKLTRrIxw.png\" height=\"40%\" width=\"40%\">\n",
    "<img src=\"1.jpg\" height=\"70%\" width=\"70%\">\n",
    "<img src=\"2.jpg\" height=\"70%\" width=\"70%\">\n",
    "\n",
    "##### here if get positive slope by calculating derivative then we subtract slope*alpha from theta 0 \n",
    "##### if we get negative slope by calculating derivative then we add slope*alpha to theta 0\n",
    "##### here we can face the problem of local minima  , but cost function will be recover from here but in DL we use adam or RMSProp optimizert to solve this problem \n",
    "##### local minima problem\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*uAh75dLKI8A9wysKUG9jYg.png\" height=\"40%\" width=\"30%\">\n",
    "\n",
    "##### in case of multi-variate linear regression , gradient descent curve will look like\n",
    "<img src=\"3.jpg\" height=\"70%\" width=\"70%\">\n",
    "\n",
    "##### here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul Agrawal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Rahul Agrawal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\Rahul Agrawal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Importing and loading model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting data to model\n",
    "\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Predicting the test set results and error calculation\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the best fit line \n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# visualizer = PredictionError(model)\n",
    "# visualizer.fit(x_train, y_train)  \n",
    "# visualizer.score(x_test, y_test)  \n",
    "# visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------- <b> 1.1.1.b  Polynomial Regression Model  </b> ------------------\n",
    "\n",
    "##### formula is given by <h1> h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub>   +  θ<sub>2</sub>x<sub>2</sub>  <sup>2</sup>   +  θ<sub>3</sub>x <sub>3</sub> <sup>3</sup> + - - -  +    θ<sub>n</sub>x <sub>n</sub> <sup>n</sup></h1>\n",
    "<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/machine-learning-polynomial-regression.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Fitting the Polynomial regression to the dataset  \n",
    "from sklearn.preprocessing import PolynomialFeatures  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly_regs= PolynomialFeatures(degree= 2)  \n",
    "# x_poly= poly_regs.fit_transform(x)  \n",
    "# lin_reg_2 =LinearRegression()  \n",
    "# lin_reg_2.fit(x_poly, y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.1.2 Ridge and lasso Regression  </b> ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case of Ridge and Lasso , try both and based on performance matrix choose the best one\n",
    "##### Ridge is regualarized regression model that is used to solve the problem of overfitting.\n",
    "##### Ridge regularization is also known as L2 regularization.\n",
    "##### As in case of Overfitting , cost function is nearly equal to 0 so model stop learing trends in dateset , so to solve this problem here nre term is added to the cost function i.e.\n",
    "<h1>J(<b>θ</b><sub>1</sub>) = 1/2n *  Σ[ (h(x<sub>i</sub>) - y<sub>i</sub>)²  + λ*(<b>θ</b><sub>1</sub>)²  ] </h1>\n",
    "\n",
    "##### here λ is the regularization parameter and it is used to control the amount of regularization , found by cross validation/try-and-found . because of  λ(<b>θ</b><sub>1</sub>)² if slope is higher then it plays crucial role but if slope is getting low then it is not so important , casue most of the time stip sloped line leads to overfitting and flat line leads to underfitting , best fit line should be fairly stip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV   # used for hyperparameter tuning      , Not mandatory\n",
    "\n",
    "param_grid = {'alpha': [1e-10,1e-5,0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "ridge_model_hyper = GridSearchCV(ridge_model, param_grid , cv=5 , scoring='neg_mean_squared_error')\n",
    "\n",
    "# ridge_model_hyper.fit(x_train, y_train)\n",
    "\n",
    "print(\"parameter that gave best results are : \",ridge_model_hyper.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge_model.fit(x_train, y_train)\n",
    "\n",
    "# y_pred = ridge_model.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the best fit line \n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# visualizer = PredictionError(ridge_model)\n",
    "# visualizer.fit(x_train, y_train)  \n",
    "# visualizer.score(x_test, y_test)  \n",
    "# visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso is also regualarized regression model that is used to solve the problem of overfitting.\n",
    "##### Lasso regularization is also known as L1 regularization.\n",
    "##### As in case of Overfitting , cost function is nearly equal to 0 so model stop learing trends in dateset , so to solve this problem here nre term is added to the cost function i.e.\n",
    "\n",
    "<h1>J(<b>θ</b><sub>1</sub>) = 1/2n *  Σ[ (h(x<sub>i</sub>) - y<sub>i</sub>)²  + λ( |<b>θ</b><sub>1</sub>| )  ] </h1>\n",
    "<h5> |<b>θ</b><sub>1</sub>| helps in feature selection and helps in removing the least important features </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV   # used for hyperparameter tuning      , Not mandatory\n",
    "\n",
    "param_grid = {'alpha': [1e-10,1e-5,0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "ridge_model_hyper = GridSearchCV(ridge_model, param_grid , cv=5 , scoring='neg_mean_squared_error')\n",
    "\n",
    "# ridge_model_hyper.fit(x_train, y_train)\n",
    "\n",
    "print(\"parameter that gave best results are : \",ridge_model_hyper.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso.fit(x_train, y_train)\n",
    "# y_pred_lasso = lasso.predict(x_test)\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# mse = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "# print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the best fit line \n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# visualizer = PredictionError(lasso)\n",
    "# visualizer.fit(x_train, y_train)  \n",
    "# visualizer.score(x_test, y_test)  \n",
    "# visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing best regularization parameter using Performance Metrics\n",
    "##### it includes Mean Absolute Error (MAE) Mean Squared Error (MSE) Root Mean Squared Error (RMSE) R-Squared.\n",
    "\n",
    "<img src=\"5.jpg\" height=\"40%\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.1.3 SVM  </b> ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.1.3 SVM  </b> ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _____________________________ 1.2 Classification _____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification is the process of classifying objects into groups based on the values of the features.\n",
    "#### it can be binary classification or multi-class classification\n",
    "#### Confusion Matirx is a table that is used to evaluate the performance of a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.2.1 Logistic Regression  </b> ------------------\n",
    "\n",
    "\n",
    "##### Here we try to squash the best fit line from end points of that line in order divide it into 2 classes , that's why it is known as logistic regression \n",
    "\n",
    "##### <h1> h<sub>θ</sub>(x) =g( θ<sub>0</sub> + θ<sub>1</sub>x  )    </h1>\n",
    "##### here g is sigmoid/logistic function and it is used to squash the best fit line from end points of that line in order divide it into 2 classes\n",
    "\n",
    "##### we can say that h<sub>θ</sub>(x) =g(Z)  , where Z =  θ<sub>0</sub> + θ<sub>1</sub>x\n",
    "##### then by applying the function we get ,\n",
    "<h1> h<sub>θ</sub>(x) = 1 / (1 + e<sup>-Z </sup>)  </h1>\n",
    "\n",
    "\n",
    "<img src=\"6.jpg\" height=\"60%\" width=\"40%\">\n",
    "\n",
    "<h3> h<sub>θ</sub>(x) = 1 / (1 + e<sup>-(θ<sub>1</sub>x)  </sup>)  </h3> Considering θ<sub>0</sub> as 0\n",
    "\n",
    "##### but it non-convex function so it leads to local minima problem , so we use some modification to make it convex\n",
    "<img src=\"https://miro.medium.com/max/926/1*ZyjEj3A_QyR4WY7y5cwIWQ.png\" height=\"60%\" width=\"40%\">\n",
    "<img src=\"https://miro.medium.com/max/996/1*TqZ9myxIdLuKNmt8orCeew.png\" height=\"60%\" width=\"40%\">\n",
    "<img src=\"7.jpg\" height=\"60%\" width=\"40%\">\n",
    "\n",
    "### Before using classification , check data is balanced or not , if not then we need to balance it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "log_model =  LogisticRegression( solver='lbfgs' , max_iter=1000 , penalty='l2' , C=1)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "#  here \n",
    "\n",
    "#  solver is the algorithm used to solve the optimization problem\n",
    "#  max_iter is the maximum number of iterations for the solver to run\n",
    "# C is the inverse of regularization strength; must be a positive float.\n",
    "# penalty is the type of regularization used. l1 is the lasso penalty,l2 is ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We use confusion matrix to evaluate the performance of a classification model.\n",
    "##### True Positive: Interpretation: You predicted positive and it’s true.\n",
    "##### True Negative: You predicted negative and it’s true.\n",
    "##### False Positive: (Type 1 Error)   You predicted positive and it’s false.\n",
    "##### False Negative: (Type 2 Error)  You predicted negative and it’s false.\n",
    "\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781838555078/graphics/C13314_06_05.jpg\" height=\"40%\" width=\"40%\">\n",
    "\n",
    "### We get Accuracy as : True Positive + True Negetive  /   Total samples\n",
    "## Recall     (For type 2 error)                \n",
    "###   True Positive / (True Positive + False Negative)\n",
    "#### ​Recall: Out of total actual positives, how many predicted are positive .Recall/sensetivity should be high as possible.\n",
    "#### used where higher False Negetive value will be disaster  \n",
    "##### it is considerd priorly in a case like canser detection as having canser but getting false result leads to heavy cost.\n",
    "\n",
    "## Precision      (For type 1 error)                     \n",
    "###  True Positive / (True Positive + False Positive)   \n",
    "#### from all the classes we have predicted as positive, how many are actually positive. Precision should be high as possible.\n",
    "#### used where higher False Positive value will be disaster  \n",
    "##### it should be given priority in a case like spam email \n",
    "\n",
    "### F-Measure-Score is calculated as : (1 + β² ) * (Precision * Recall) /  β²(Precision + Recall)\n",
    "#### It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more\n",
    "### in case of F1-score , β = 1 ,i.e. F1-score = (2 * Precision * Recall) / (Precision + Recall)\n",
    "#### if False Positive is more important then we keep  β  value as minimum i.e. β = 0.5 \n",
    "#### if False Negetive is more important then we keep  β  value between 2 to 10 based on impact of false negetive value\n",
    "\n",
    "### We can do this using classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.2.2 Naive bais  </b> ------------------\n",
    "\n",
    "##### naive bais is a simple model that assumes that the features are independent of each other.\n",
    "##### it is based on bayes theorem and it is used to classify the data. it is given by :\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172-300x172.png\">\n",
    "\n",
    "#### here P(A) := y = no  and P(B) := P(xi)    i.e  x= 0  to x=i   , here only P(xi) will be constant as it will be ant way canceled in further calculation\n",
    "<img src=\"8.jpg\">\n",
    "\n",
    "##### same as above we calculate P(A) := P(y=yes/x1 to x4)  \n",
    "\n",
    "##### after that  P(yes) = P(y=yes/x1 to x4)  /  P(y=yes/x1 to x4) + P(y=no/x1 to x4)   \n",
    "#####  same with P(no) = P(y=no/x1 to x4)  /  P(y=yes/x1 to x4) + P(y=no/x1 to x4)  but it will be  P(no) = 1 - P(yes) , and which on ein greater , it will be answer\n",
    "##### \n",
    "##### \n",
    "##### \n",
    "##### \n",
    "##### \n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.2.3  k Nearest Neighbour  KNN </b> ------------------\n",
    "\n",
    "\n",
    "##### using Euclidean distance to calculate the distance between two points\n",
    "#####  also we can use manhattan distance to calculate the distance between two points\n",
    "##### \n",
    "##### \n",
    "##### \n",
    "##### \n",
    "##### \n",
    "##### \n",
    "##### \n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.2.4  Decision Tree </b> ------------------\n",
    "\n",
    "##### Decision Tree is a tree-based model for classification and regression.\n",
    "#####  here we have nodes which contains the feature and the value of that feature \n",
    "##### A pure node is node having only one class label. like if we have only two classes then pure node will be having only one class labels.\n",
    "#### Entropy \n",
    "##### Entropy is a measure of how often a randomly chosen element from a set will be labeled as a certain class.\n",
    "##### Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.\n",
    "##### it is given by :\n",
    "<img src=\"https://miro.medium.com/max/782/1*nNY_7_aWRwp8E2DyGduEPg.png\">\n",
    "\n",
    "##### for binary classification , we have 2 classes i.e. yes and no , so entropy is given by :\n",
    "##### H (Entropy) = [ -(P(yes) * log2(P(yes)) ]  - [ P(no) * log2(P(no))) ]\n",
    "##### entropy is zero for pure node , higher the entropy value , more the disorder/impurity in the data\n",
    "##### it generally in between 0 and 1 , but can be greater than 1 and then also it is considerd as 1 \n",
    "#### Gini Impurity\n",
    "#####\n",
    "#####\n",
    "#### Information Gain \n",
    "#####  it helps to decide which feature is the best feature to split the data or which feature will acts as root node and parent node of other nodes.\n",
    "##### Information Gain, or IG for short, measures the reduction in entropy or surprise by splitting a dataset according to a given value of a random variable.\n",
    "###  IG(S ,x) = H(x) -  Σ<sub>v</sub> [ ( |S<sub>v</sub>| / |S| ) *H(x<sub>v</sub>) ]      ---  x is root node , S is feature value of root node\n",
    "##### V belongs to catagory i.e. child node of parent node , Sv is count of childs split count from parents value of feature\n",
    "##### e.g  suppoes we have 2 classes i.e. yes and no , parent node S and 2 childs c1 and c2 \n",
    "##### IG is calculated as given by with feature x we have 9yes and 5 no , c1 has 6yes , 2no and c2 has 3yes , 3no\n",
    "##### H(x) = -(9/14) * log2(9/14) + (5/14) * log2(5/14) = 0.94\n",
    "##### H(c1) = -(6/8) * log2(6/8) + (2/8) * log2(2/8) = 0.81\n",
    "##### H(c2) = -(3/5) * log2(3/5) + (3/5) * log2(3/5) = 1\n",
    "#####  IG(S ,x) = 0.94 - [ 8/14 * 0.81 +  6/14 * 1 ] = 0.041    ---- sv = 8 and 6 as 8 values belongs to c1 and 6 values belongs to c2 out of 14 values\n",
    "#### Gini Impurity \n",
    "#####\n",
    "##### in case of pure nodes gini impurity is 0.5 , entropy is 1\n",
    "##### entropy is costly to calculate as it has log function , so we use gini impurity\n",
    "#####\n",
    "#####\n",
    "#####\n",
    "#####\n",
    "#####\n",
    "#####\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _____________________________ 1.3 Ensemble Techniques _____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble techniques are a set of algorithms that combine the power of individual learning algorithms to improve their predictive performance.\n",
    "##### They combine the decisions from multiple models to improve the overall performance. \n",
    "#####\n",
    "#####\n",
    "#####\n",
    "##### There are 2 types of Ensemble techniques :\n",
    "### 1. Bagging\n",
    "#####  here we use bootstrap aggregating technique to get avarage of predicative performance of combination of individual models.\n",
    "#####  here we have random forest\n",
    "#####  \n",
    "#####  \n",
    "### 2. Boosting\n",
    "##### here we use various model at a time to get better result , it is sequential set of combined models .\n",
    "##### here we use adaboost , gradient boost , extream boost , \n",
    "#####\n",
    "#####\n",
    "#####\n",
    "#####\n",
    "#####\n",
    "#####\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## 2. Unsupervised Learning\n",
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### ____________________________ 2.1 Clustering _____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mainly used for customer segmentation and data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### ____________________________ 2.2  Dimentionality Reduction _____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### used to reduce the dimension of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## 3. Semi-Supervised Learning\n",
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## 4. Reinforcement Learning\n",
    "__________________________"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbcae02c4273700af05fd878483d93fcaab7791578566c35fbbd6258eb8c5fe6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
