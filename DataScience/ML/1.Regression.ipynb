{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Regression Algorithms \n",
    "### Linear Regression \n",
    "###  Ridge and Lasso\n",
    "### SVM \n",
    "### Decision Tree \n",
    "###  Bayesian Ridge\n",
    "###  KNN\n",
    "###  OLS \n",
    "\n",
    "________________________________________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "df= load_boston()\n",
    "target = df.target\n",
    "df = pd.DataFrame(df.data, columns=df.feature_names)\n",
    "df  = pd.concat([df, pd.DataFrame(target, columns=['target'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### ____________________________ 1.1 Regression _____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Also known as estimation model\n",
    "#### regression is the process of predicting a continuous value\n",
    "##### Performance Metrics is used in case of regression\n",
    "##### if gradient descent is invilved in model then use Standerd scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ****      Regression Algorithms         ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ---------- <b> 1. Linear Regression  </b> ------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear regression is a simple linear model that assumes that the relationship between the input variables and the output variable is linear.\n",
    "##### here we try to find the best fit line for the data , here y is linear function of x , we predict y based on values of x and parameters of the line\n",
    "##### here we use equation of the line to predict y i.e. y = mx + c   _OR_  y = (Mi)(Xi) + C   where i is the index of the input variable\n",
    "##### it is writen in standerd terms as <h1> h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x      </h1>\n",
    "#####  where m is the slope(unit movement in x axis then respective movement in y axis) of the line and c is the intercept (at what point does the line intersect the y-axis)\n",
    "##### here we try to minimize the distance between the data points and the line that is known best-fit line\n",
    "##### here we calculate the cost function\n",
    "### <h1>J(<b>θ</b><sub>0</sub>,<b>θ</b><sub>1</sub>) = 1/2n *  Σ(h(x<sub>i</sub>) - y<sub>i</sub>)²   </h1> &nbsp;  <sub> theta_0 = C , theta_1 = M , h(x<sub>i</sub>) = predicted/calculated value ,  y<sub>i</sub> = Actual value </sub>\n",
    "#####  it is required to update values of slope and c in order to minimize the error  , summation is considerd from i=1 to i=n , also  h(xi) is predicted value of y for xi and yi is actual value of y for xi  --- we divided here by 1/2n because by 1/n we get avarage of error  and 1/2 for differentiate helper\n",
    "#####   according to the cost function we can plot the graph of cost function and find the minimum value of cost function , it is also called gradient descent algorithm , here we take random value once of slope and c and find update it and finds the minimum value of cost function  \n",
    "\n",
    "##### to update the slope and c we use the formula known as convergence algorithm(Repeat until convergence)\n",
    "### Repeat until convergence {\n",
    "#####   <b>θ</b><sub>j</sub> = <b>θ</b><sub>j</sub> - α * δ/δ<sub><b>θ</b><sub>j</sub></sub> J( <b>θ</b><sub>0</sub>, <b>θ</b><sub>1</sub>)  \n",
    "### }\n",
    "#####    α is learning rate , δ is partial derivative of cost function with respect to theta 0 and theta 1\n",
    "##### If we choose our learning rate too large, then there is a problem of overshooting and not achieving our desired convergence point(global minimum point). \n",
    "##### II) At, the same time if we choose our learning rate to be too small, then it may take a lot of time to get to the convergence point, which is not very time consuming, but we still could reach to our global minimum.\n",
    "<img src=\"https://miro.medium.com/max/1400/1*o95nDGY2oV9r3jKLTRrIxw.png\" height=\"40%\" width=\"40%\">\n",
    "<img src=\"1.jpg\" height=\"70%\" width=\"70%\">\n",
    "<img src=\"2.jpg\" height=\"70%\" width=\"70%\">\n",
    "\n",
    "##### here if get positive slope by calculating derivative then we subtract slope*alpha from theta 0 \n",
    "##### if we get negative slope by calculating derivative then we add slope*alpha to theta 0\n",
    "##### here we can face the problem of local minima  , but cost function will be recover from here but in DL we use adam or RMSProp optimizert to solve this problem \n",
    "##### local minima problem\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*uAh75dLKI8A9wysKUG9jYg.png\" height=\"40%\" width=\"30%\">\n",
    "\n",
    "##### in case of multi-variate linear regression , gradient descent curve will look like\n",
    "<img src=\"3.jpg\" height=\"70%\" width=\"70%\">\n",
    "\n",
    "##### here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul Agrawal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Rahul Agrawal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\Rahul Agrawal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Importing and loading model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting data to model\n",
    "\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Predicting the test set results and error calculation\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the best fit line \n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# visualizer = PredictionError(model)\n",
    "# visualizer.fit(x_train, y_train)  \n",
    "# visualizer.score(x_test, y_test)  \n",
    "# visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression with least squares method\n",
    "\n",
    "#### slope of the line is calculated by using the formula\n",
    "\n",
    "####  m = ( n.Σ(xy) - ΣxΣy ) / ( n.Σ(x²) - (Σx)² )          ---n is the number of data points.\n",
    "\n",
    "#### bias(b)  = ( Σ(y) - m.Σx ) / n                        ---- m is calculated slope of the line \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------- <b> 1(b).  Polynomial Regression Model  </b> ------------------\n",
    "\n",
    "##### formula is given by <h1> h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub>   +  θ<sub>2</sub>x<sub>2</sub>  <sup>2</sup>   +  θ<sub>3</sub>x <sub>3</sub> <sup>3</sup> + - - -  +    θ<sub>n</sub>x <sub>n</sub> <sup>n</sup></h1>\n",
    "<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/machine-learning-polynomial-regression.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Fitting the Polynomial regression to the dataset  \n",
    "from sklearn.preprocessing import PolynomialFeatures  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly_regs= PolynomialFeatures(degree= 2)  \n",
    "# x_poly= poly_regs.fit_transform(x)  \n",
    "# lin_reg_2 =LinearRegression()  \n",
    "# lin_reg_2.fit(x_poly, y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 2. Ridge and lasso Regression  </b> ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case of Ridge and Lasso , try both and based on performance matrix choose the best one\n",
    "##### Ridge is regualarized regression model that is used to solve the problem of overfitting.\n",
    "##### Ridge regularization is also known as L2 regularization.\n",
    "##### As in case of Overfitting , cost function is nearly equal to 0 so model stop learing trends in dateset , so to solve this problem here nre term is added to the cost function i.e.\n",
    "<h1>J(<b>θ</b><sub>1</sub>) = 1/2n *  Σ[ (h(x<sub>i</sub>) - y<sub>i</sub>)²  + λ*(<b>θ</b><sub>1</sub>)²  ] </h1>\n",
    "\n",
    "##### here λ is the regularization parameter and it is used to control the amount of regularization , found by cross validation/try-and-found . because of  λ(<b>θ</b><sub>1</sub>)² if slope is higher then it plays crucial role but if slope is getting low then it is not so important , casue most of the time stip sloped line leads to overfitting and flat line leads to underfitting , best fit line should be fairly stip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV   # used for hyperparameter tuning      , Not mandatory\n",
    "\n",
    "param_grid = {'alpha': [1e-10,1e-5,0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "ridge_model_hyper = GridSearchCV(ridge_model, param_grid , cv=5 , scoring='neg_mean_squared_error')\n",
    "\n",
    "# ridge_model_hyper.fit(x_train, y_train)\n",
    "\n",
    "print(\"parameter that gave best results are : \",ridge_model_hyper.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge_model.fit(x_train, y_train)\n",
    "\n",
    "# y_pred = ridge_model.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the best fit line \n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# visualizer = PredictionError(ridge_model)\n",
    "# visualizer.fit(x_train, y_train)  \n",
    "# visualizer.score(x_test, y_test)  \n",
    "# visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso is also regualarized regression model that is used to solve the problem of overfitting.\n",
    "##### Lasso regularization is also known as L1 regularization.\n",
    "##### As in case of Overfitting , cost function is nearly equal to 0 so model stop learing trends in dateset , so to solve this problem here nre term is added to the cost function i.e.\n",
    "\n",
    "<h1>J(<b>θ</b><sub>1</sub>) = 1/2n *  Σ[ (h(x<sub>i</sub>) - y<sub>i</sub>)²  + λ( |<b>θ</b><sub>1</sub>| )  ] </h1>\n",
    "<h5> |<b>θ</b><sub>1</sub>| helps in feature selection and helps in removing the least important features </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV   # used for hyperparameter tuning      , Not mandatory\n",
    "\n",
    "param_grid = {'alpha': [1e-10,1e-5,0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "ridge_model_hyper = GridSearchCV(ridge_model, param_grid , cv=5 , scoring='neg_mean_squared_error')\n",
    "\n",
    "# ridge_model_hyper.fit(x_train, y_train)\n",
    "\n",
    "print(\"parameter that gave best results are : \",ridge_model_hyper.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso.fit(x_train, y_train)\n",
    "# y_pred_lasso = lasso.predict(x_test)\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# mse = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "# print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the best fit line \n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# visualizer = PredictionError(lasso)\n",
    "# visualizer.fit(x_train, y_train)  \n",
    "# visualizer.score(x_test, y_test)  \n",
    "# visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net\n",
    "# Sometimes, the lasso regression can cause a small bias in the model where the prediction is too dependent upon a particular variable. \n",
    "# In these cases, elastic Net is proved to better it combines the regularization of both lasso and Ridge. \n",
    "# The advantage of that it does not easily eliminate the high collinearity coefficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 3. SVM (SVR)  </b> ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 4. OLS  </b> ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 5. KNN  </b> ------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b82bd812b6a37d04ba59de08f4dfa67a262de1c3e27303c0cb0a5b38b381c61f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
