Statistics is a branch of mathematics dealing with the collection, analysis, interpretation, and presentation of masses of numerical data. It is basically a collection of quantitative data.
it is used for better decision making , it is nothing but science used to collect , organize and analyze data
data is fact or piece of information that can be measured , also can be COLLECTION OF FACTS N FIGURES OR   Information related to objext

Types of Statistics –

1.Theoretical Statistics
2.Applied Statistics  => this is further divided into  i] Descriptive statistics	     ii] Inferential statistics


  i] Descriptive statistics
	   # Descriptive statistics is a term given to the analysis of data that helps to describe, show and summarize data in a meaningful way. It is a simple way to describe our data.
	   
           #  Types of Descriptive Statistics –
				Measure of Central Tendency  => e.g mean , mode , median
				Measure of Variability / Dispersion => e.g. variance , standerd deviation , range
				Measures of Position    =>	Percentile Ranks, Quartile Ranks
				 Measures of Frequency   =>	Count, Percent, Frequency

	    #  This type of statistics is applied on already known data.

		# present raw data ineffective/meaningful way using numerical calculations or graphs or tables.

		#  With descriptive statistics, there is no uncertainty because you are describing only the people or items that you actually measure

		# e.g mean , SD , mode of marks of class


  ii] Inferential statistics

	# inferential statistics predictions are made by taking any group of data in which you are interested. It can be defined as a random sample of data taken from a population to 				describe and make inference about the population.

	# it is used to form a conclusion on the basis of data(sample) that represent larger data(population)

	# It basically allows you to make predictions by taking a small sample instead of working on whole population

	# sample is choosen using diff. sampling techniques like 

		1.Simple Random sampling  =>choosen completely random ,  skipping the repeated number because we do not survey or interview the same person twice

		2.Systematic sampling => easier to do than random sampling. In systematic sampling, the list of elements is "counted off". That is, every kth element is taken

		3.Convenience sampling => worst technique to use. In convenience sampling, readily available data is used. That is, the first people the surveyor runs into.
					e.g. only expert of cloud tech are allowd to participate in survey based on clud tech

		4.Cluster sampling => accomplished by dividing the population into groups,usually geographically. clusters are randomly selected, and each element in the selected clusters 					are used. 

		5.Stratified sampling => divides the population into groups by some characteristic called strata ,For instance, the population might be separated into males and females. A 					sample is taken from each of these strata using either random, systematic, or convenience sampling.
					strata is always non-overlapping , cluster may have overlapping data , in strata one sample group is completely diff. from other strata group
					 e.g. strata is sample of male and female different but cluster is sample such as peoples lives in perticular area 

	#  Because the goal of inferential statistics is to draw conclusions from a sample and generalize them to a population, we need to have confidence that our sample accurately 				reflects the population

	# The most common methodologies in inferential statistics are hypothesis tests, confidence intervals, and regression analysis.

	# The following types of inferential statistics are extensively used and relatively easy to interpret:

		1.One sample test of difference/One sample hypothesis test
		2.Confidence Interval
		3.Contingency Tables and Chi Square Statistic
		4.T-test or Anova
		5.Pearson Correlation
		6.Bi-variate Regression
		7.Multi-variate Regression
		8.Hypothesis Testing
		9.Normal Distributions
		10.Central Limit Theorem
		11.Comparison of Means.

	# e.g. how likely the student is going to pass based on avg. marks of the class of that student and marks required for passing

	#  kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable.Kernel density estimation is a fundamental data 		smoothing problem where inferences about the population are made, based on a finite data sample.


e.g. 100 peoples are asked that they whether like pizza or not , based on that 
	1. we made a histogram of yes and no answers , this is descriptive
	2. on the basis of that data , we made a conclusion that major chunk of population like pizza , this is inferential
	3. we compared pizza data with burger data , found out avg./mean rating(discriptive) and based on that we conclude that peoples 
		likes pizza more than burger ,this is also inferential




To perform statistical analysis of data, it is important to first understand variables and what should be measured using these variables.

 // **  Variable ** //  

	it is property that can take any value like a variable can take diff. values with respect to age of person.
	
	mainly 2 types of variable 
	
		1. Quantitative variable => it can be measured numerically or can say magnitudely like age, height of person , we can perform also math operations like +,- etc.
						it can be used to plot graphs and has meaning in it without comparison

			Quantitative variables are divided into two types: discrete and continuous.

				i] discrete => values it can take are countable and have a finite number of possibilities. e.g total citizens in region
 
			       ii] Continuous => variables for which the values are not countable and have an infinite number of possibilities. e.g weight , rainfall

		2. Qualitative/catagorical Variable => based of characteristics it lies in catagories like gender of person , it is variables that are not numerical and which values fits 						into categories.it is a variable which takes as its values modalities, categories or even levels, we cant perform math operations here

			e.g. pincode , year , phone no. etc 
			
			Some variables, such as social security numbers and zip codes, take numerical values, but are not quantitative: They are qualitative or categorical variables. The 			sum of two zip codes or social security numbers is not meaningful. The average of a list of zip codes is not meaningful.

		qualitative variable with exactly 2 levels is also referred as a binary or dichotomous variable.

			it is further divided into typs

				i] Nominal => no ordering is possible or implied in the levels. For example gender is nominal because there is no order in the levels female/male

				ii] Ordinal => variable with an order implied in the levels. For instance, if the severity of road accidents has been measured on a scale such as light, 						moderate and fatal accidents Or health, which can take values such as poor, reasonable, good, or excellent.
				
					it is value that can be measued from low to high severity , value does not matter here like ranking a student on the basis on marks ,
					 but marks doent matter much only rank that is order matters

	

The level of measurement of a variable decides the statistical test type to be used. The mathematical nature of a variable or in other words, how a variable is measured is considered as the level of measurement.

There are 4 tyeps :

	1. Nomianal => e.g where do you live in town or village

	2. ordinal => e.g. how do you feel like not well , moderate , highly energatic , only order has imp role

	3. interval => e.g. rate your experiance with movie from 1 to 10 star , order and value both have imp. role , natural zero is absent always
			another e.g. tempreture intervals of 0-20 degree , here zero has also meaning not like mathamatical zero which means nothing 
			here numbers are assigned to objects such that the differences (but not ratios) between the numbers can be meaningfully interpreted

	4. Ratio =>  have all the attributes of interval scale variables and one additional attribute ratio scales include an absolute “zero” point.
			natural zero is present 
			For example, traffic density (measured in vehicles per kilometer) represents a ratio scale. The density of a link is defined as zero when there are no 
			vehicles in a link. Other ratio scale variables include number of vehicles in a queue, height of a person, distance traveled, accident rate, etc



//////   ***   Types Of Charts/Graphs   ***   ///////

	1. Bar Chart => A bar chart consists of a grid and some vertical or horizontal columns (bars). Each column represents quantitative data.values are discrete

	2. Box Plots  =>  A box plot displays summary statistics for the distribution of values for a variable. The outer bounds of the box represent the first and third quartiles.
			 The line inside the box represents the median. The markers outside the box, referred to as outliers,
			 represent data points that are outside of the 25th and 75th percentiles.

	3. Histograms   => A histogram is a bar chart that displays the observed frequencies of data that have been binned (divided into contiguous, equally spaced intervals).
			   The heights of the bars indicate the relative frequency of observations in each bin , bin is placed on x axis
				it tells share of data falls in perticular interval of bin , values are continuous , KDE is used to smoothening histogram

	4. Pie Charts  => A pie chart is a circular chart that is divided into slices by radial lines. Each slice represents the relative contribution of each part to the whole.

	5. Scatter Plots => A scatter plot is a two- or three-dimensional plot that shows the joint variation of two (or three) variables from a group of observations. The coordinates of 			each point in the plot correspond to the data values for a single observation.



//////   ***   Measure of Central Tendency    ***   //////   

	mean  
		mean is the arithmetic average of a set of given numbers. 

	median
		The median is the middle score in a set of given numbers. sort in ascending order and middle will be median , 
			for odd count it is middle , for even count take avg of 2 middle elements

	mode 
		The mode is the most frequently occurring score in a set of given numbers.

				*analysts tend to use the mean because it includes all of the data in the calculations. However, if you have a skewed distribution, the median is often the 						best measure of central tendency.



//////   ***   Measure of Variability / Dispersion(spread)    ***   //////   

	used to analyze how 2 distributions are differ from each other

	Variance: 
			Variance is a numerical value that shows how widely the individual figures in a set of data distribute themselves about the mean. That is how far
			 each number is from the mean, and thus from each other. A variance of zero value means all the data are identical.

			

		S^2 (sample variance square) = Summation{ [(x_i) - X ]^2 } / n-1         ----- in case on sample variance

					x_i 	=	the value of the one observation 
					(X)	=	the mean value of all observations , diff for sample and population
					n	=	the number of observations

		difference between population variance and sample variance relates to calculation of variance

			For Population of 1000 , n is taken as 1000 for calculating variance , but in case of sample of 100 n is taken is 100 - 1 for variance 
			population means when we have all data about that , but most of the time it is just sample of population
			n-1 is used because there is no variation if there is only 1 observation 

		Due to this, the value of variance calculated from sample data is higher than the value that could have been found out by using population data. The logic of doing that is 				to compensate our lack of information about the population data.
		
		A large variance indicates that numbers in the set are far from the mean and far from each other , high Variance can lead to overfitting


	standard deviation 

		it is the square root of the population variance.  it is way to express the value is how far from the mean in terms standerd deviation times. 
		 SD is a measure of how much any random data point is varying or distant from the mean of all the data points from where you have picked that random point.




//////   ***   Measures of Position    ***   //////   

	Percentile 

			it is value below which certain percentage of observation lies

			
			percentile of value x = [(no. of values lies below x) / (total sample size) ]*100       --- dont consider x itself while calculating below values


			e.g.  for dataset 1,5,6,7,8,9  => percentile of 7 = 3/5 * 100 = 60 percentile , it means 60 percent of total values are less than 7

			index of value(X) = [ percentile of X / 100 ] * (total sample + 1)          ---- index in terms of after sorted order of all data , 
											if index is float then value = value at index(X) + value at index(X+1) / 2

	

	Quatile

			A quantile defines a particular part of a data set, i.e. a quantile determines how many values in a distribution are above or below a certain limit. Special 			quantiles are the quartile (quarter), the quintile (fifth) and percentiles (hundredth).
		
		main 5 terminologies in IQR (inter quartile range which is used to identify outlier from dataset)
		
			1. minimum    ( q1 - 1.5(IQR) )						---- IQR = q3 -q1
			2. first quartile (q1) , 25 percentile
			3. median
			4. thirs quartile (q3) , 75 percentile
			5. maximum     ( q3 - 1.5(IQR) )

				value beyond minimum and maximum considerd as outlier , only minimum < VAlUE < maximum considerd as valid , Box Plot is drawn based on this values

		get percentile using , np.percentile(df[col] ,[25,50,75]) ==> it will return array with three diff percentile values for that specific column



//////   ***   distribution    ***   //////   


		The distribution is a mathematical function that describes the relationship of observations of different heights. A distribution is simply a collection of data, or scores, 		on a variable. Usually, these scores are arranged in order from smallest to largest and then they can be presented graphically.



	1. normal/Gaussian distribution  (sns.histplot(df[col]) used to see distribution , use kde=true for also view the PDF)

			when plotted on graph , mean has maximum on that graph and also it follows bell curve means it is symmetric anout mean , it is known as normal distribution.
				S.D determines the amount of dispersion away from the mean. A small standard deviation (compared with the mean) produces a steep graph, whereas a large 					standard deviation (again compared with the mean) produces a flat graph. 
			in case of normal distrubution always mean = median = mode    --- all are equal 





													for noramal distribution
		------------|--------|--------|--------|--------||--------|--------|--------|--------|-----------   
					4sd	    3sd	     2sd     1sd       mean      1sd	  2sd     3sd       4sd


												<----- 68 % ------>     this amount of data from total data lies in this region

										<------------- 95 % --------------->  68–95–99.7 rule, also known as the empirical rule

								<--------------------- 99.77 % ---------------------->  
				68%, 95%, and 99.7% of the values lie within one, two, and three standard deviations of the mean



		 Z-score is a numerical measurement that describes a value's relationship to the mean of a group of values . 
		 for mean , z-score =0 

Formula =>  Z-Score-of-datapoint = (datapoint - mean of sample ) / SD of sample           ----squareroot(n) is 1 for a single datapoint

		z-score is a standard score that tells you how many standard deviations away from the mean an individual value (x) lies:

					A positive z-score means that your x-value is greater than the mean.
					A negative z-score means that your x-value is less than the mean.
					A z-score of zero means that your x-value is equal to the mean.

	2. Standerd Normaml Distribution

		special case of normal distribution , where after applying z-score , we got mean as 0 and SD as 1 , it is known as STD normal / Z distribution 

		this concept is used to standerdize data in sklearn(standerd scaler) , where a column is taken , z-score is applied wrt to datapoint value 
											mean has value 0 and applied value will be less if value is nearer to mean
		standerdization is used when feature distribution is Normal or Gaussian.

		Normalization is also standerdiztion with range , basically is normalization , values will be converted into scale of 0 to 1 , or any defined
		minmax scaler is normaliztion technique , highly affected by outliers , use only if we dont know about features of distribution
		
		
	3. Log normal distribution :
	
	
		whenever log is applied , it is converted into gaussian/normal distribution
	
		
		
	4. Bernoulli distribution :
	
	
	5. pareto distribution :
	
		
		
		




********************************************************************   Probability ****************************************************************************


Mutually Exclusive = only one event occures at a time , e.g. no. getting after rolling a die or toss a coin

Non Mutual Exclusive : 1 or more event can occures at a time , like 1 random card is picked it lies in 4 types as well as in 13+3 combination at a same time

					P(a U b) = P(a) + P(b) - P(a ∩ b)      ,  U is Or & ∩ is And operation

Dependent Probabilty = When probability of 1 event is influenced by by other , like picking a specific colored marble from set of diff. colored marbles 

Independent Probalbilty = when probability of 1 event completly random without anyones's influance , like rolling a dice getting a random no. or toss a coin

Conditional probability is defined as the probability of an event A, given that another event B has already occurred (i.e. A conditional B)

			P(A|B) = P(A ∩ B) / P(B)  , here event a is occuring which is influenced by event b which is already occured

			 P(A and B)  = P(A) * P(B/A)          And   Also   P(A and B)  =  P(B and A)

			P(A|B) = P(B|A)*P(A) / P(B)  ---baye's therom

g g g    r r      r|g  = 2/5 + 3/4  =   8 + 15/20 = 23/20   g|r  =  3/5 + 1/2 = 5 + 6 / 10 = 11/10 



********************************************************************   Permutation and combination  ****************************************************************************


 Permutation = all the possible combination can be calculated using this , nPr =n! / (n-r)!  , here combinations are repeated like (a b c) , (b a c)

Combination = only unique combinations are allowed , nCr = n! / r!(n-r)! , like (a b c) only ,not other combination is here which contains all this 3 elements 


********************************************************************  Hypothesis testing   ****************************************************************************

hypothesis is an educated guess about something in the world around you. or it is a idea that can be tested

Hypothesis testing in statistics is a way for you to test the results of a survey or experiment to see if you have meaningful results.
	e.g. a coin is said to be fair only if p(head) = p(tail) , so after testing 100 times , or million times it should be same always
Hypothesis Testing is a form of inferential statistics that allows us to draw conclusions about an entire population based on a representative sample

Null hypothesis(h0) is a hypothesis that is stating or favoring given condition but it can be false later like innocent until proven guilty. e.g. coin is fair
	it is difficult to challange null hypothesis ,  no statistical significance between the two variables in the hypothesis. it is constant 

p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.
it helps to support or reject the null hypothesis. smaller the p-value, the stronger the evidence that you should reject the null hypothesis.
p-value considerd as false negetive and considerd as randomness while testing

significance level(alpha symbol) is the probability of rejecting the null hypothesis when it is true. 0.2 significance level means 20% of false values are tolerable .
	e.g. out of 100 test , if coin is getting head 70 times , then also it is fair

Confidence Interval is a range of values we are fairly sure our true value lies in. e.g CI of 90% means , result between >5% and <95% considerd as true or null hypothesis is correct
	defined by domain expert , calculated using significance value/level i.e. CI = ( 1 - SI ) *100 %

alternate hypothesis(h1) is a everything other than null hypotheis e.g coin is not fair , coin is baised 
it is assumed here that there i co-reletion between the two variables in the hypothesis.

Experiment = test that are performed null hypothesis 

Type 1 Error : due to biasness , we reject true null hypothesis , False-Positive

Type 2 eroor : we didnt reject false null hypothesis , False-Negetive

One-Tailed Test = here significance level is just at a single side , also called directional like weight is less than mean
			if statement contains words that reseambles comparison like greater, taller , lesser then it is always one tailed

	left tailed : when a assumed score value is less than value at null hypothesis , then significance level is at left section 
			e.g. when null is packet has mean 1kg of milk , but one said that it is less than 1kg then significance level is at left side
			e.g. peoples does smoking has less chances to reach age of 60

	right tailed : when a assumed score value is greater than value at null hypothesis , then significance level is at right section 
			e.g. when null is packet has mean 1kg of milk , but one said that it is greater than 1kg then significance level is at right side
			e.g. peoples live healthy life has greater chance to reach 60 years

Two-Tailed test = significance value is equally distributed at both the sides , also called non-directional like medicine is going to affect in good or bad way
		statement contains words that is like it is going to happen but not knows in which way , like 'change','difference'
		e.g. is it a difference between IQ of young compared old peoples
		e.g. is it change in color of phone when it is day compared to night


Parametric Tests, if samples follow/assumed a normal distribution. In general, samples follow a normal distribution if their mean is 0 and variance is 1.
	e.g. ztest , ttest , ftest
		here scale of measurement is interval or ratio , assumptions are made here , data is quantitative
		mean is measure of central tendancy , more powerful and less robust

Non-Parametric Tests, if samples do not follow a normal distribution. scale of measurement is nominal or ordinal , data is qualitative
		median(positional avarage) is measure of central tendancy  , less powerful but more robust
	e.g. chi-square , htest , utest , spearman rank correlation test

/* Co-Variance 
	
		Covariance indicates the relationship of two variables whenever one variable changes.

		Cov(X,Y)=∑(Xi− x_bar )(Yi− y_bar ) / n        ----for population  , for sample n = n-1
											x_bar the mean of the X-variable

		if Cov(X,Y) is (+)ve then X is directly propotional to Y , for (-)ve it is inversaly propotional , if 0 then no relation

		covariance gives the direction but correlation gives the direction and the strength of the correlation

		correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. 
		In the broadest sense correlation is any statistical association, though it actually refers to the degree to which a pair of variables are linearly related.


	Pearson’s correlation :    -1 <= ρ <= 1  , always

			 Cov(X,Y) 				ρ is Pearson’s correlation coefficient
		ρ =     -----------
			 σx * σy                          ----- σ is SD


			if ρ is toward +1 more , strongly coreleted , ρ = 0 , no coreletion , ρ towards -1  strong inversly coreleted


	spearman correlation  =

			  cov( R(X), R(Y) )            ----R(X) is rank of x , according to acscending order what is index of value x is the rank of that value x
		S(X,Y) =  --------------------
			    σ(R(X)) * σ(R(Y))
	

************************************************************* Calculate Hypothesis test  ****************************************************
	
		1. define Null hypothesis (H0)
		2. define Alternate hypothesis (H1 i.e ~H0)
		3. state alpha value , if not given consider as 5%
		4. calculate confidence range using alpha like Z(alpha/2) ot T(alpha/2)
						interval/range as  +Z(alpha/2) to -Z(alpha/2)
		5. calculate test(t) for corresponding test
		6. state out dicision , if test(t) is  in between range , then null is failed to reject (p-value >= significance)
					otherwise reject null and accept alternate assumptions (p-value < significance-value)
						also make positive or negetive assumption as alternate based on test(t) value
						
		7. calculate p-value as , get test(t) value and find area under curve using z-score table , 
			search value according test(t) 
					if null is passed then use right z-score table , otherwise left , get value as p-value'
					
				p-value = 1 - p-value'
				
			now p-value is area under the curve (divide area for 2 tail for each section but p-value is combined both of it so keep as it is )
			
			also reamaing area will be p-value'

/* T-test : 

	used when sample size is less than 30 ,used when corelation of coefficient is zero , if not zero z-test is used
		
		confidence interval = point estimate +- margin error          --- , T(alpha/2)[ S /squareroot(n)] -> S is sample SD
							
										calculate T(alpha/2) as 
											1. if alpha is not given then -> 1 - CL
											2. calculate num = (alpha/2)
											3. find corresponding value to DF and num 
	
	there are 2 types of T-test :

		1. one sample T-test : sample mean is compared with population mean


			test (t)  =     (sample mean - population mean) squareroot(n)           ----n is sample no.
				         ------------------------------------
							sample S.D. 

			degree of freedom = sample_size(n) - 1        ----it is necessory to get value from t-distribution table
									--- it is  maximum number of logically independent values

	


		2. Two sample T-test :  mean of 1 sample is compared with another mean of another sample

	
				       (X1)^2  -  (X2)^2				X1 , X2 is mean of sample 1 and 2 resp.
			test(t) =   -----------------------------
					___________________
				       / (S1)^2      (S2)^2				S1 and S2 are SD of sample 1 and 2 resp.
				      / -------  +   ------
				    \/   n1           n2				n1 and n2 are number of sample points of sample 1 and 2 resp.


/* Z-test :

	used when population S.D. is given and sample size is greater than 30
	+ for upper bound , - for lower bound , if value lies betn this bound , then null hypothesis is accpted
	confidence interval = point estimate +- margin error				point estimate = X_bar = sample mean 
											margin error = Z(alpha/2)population S.D. /squareroot(n)  , if CL is given then alpha= 1-CL
																				CL= confidence level
				         					
										alpha/2 only if it is one tailed , if 2 tailed take only alpha

   
	* calculate Z(alpha/2) as , first get num =alpha/2 , then substract it from 1 i.e. 1-num , then find corresponding sd for value 1-num from z-score table 


	there are 2 types of Z-test :

		1. one sample Z-test : sample mean is compared with population mean


	             test (z)  =     (sample mean - population mean) squareroot(n)           ----n is sample no.   , population S.D. /squareroot(n)   = standerd error
		    			     ------------------------------------
							population S.D. 
	
		2. two sample Z-test : mean of 1 sample is compared with another mean of another sample

		
				       (X1)^2  -  (X2)^2				X1 , X2 is mean of sample 1 and 2 resp.
			test(z) =   -----------------------------
					___________________
				       / (S1)^2      (S2)^2				S1 and S2 are SD of population  1 and 2 resp.
				      / -------  +   ------
				    \/   n1           n2				n1 and n2 are number of sample points of sample 1 and 2 resp.



/* F-Test  :

		it is test for 2 normal population have same variance  , it is comparison of equality of sample variance , it also tests equality of mean

/* Annova Test :

		it is parametric test , it is extension of t and z test as here we calculate diff. of sample means are calculated for 2 or more samples

		f-stastitics = variance between sample means / variance within samples

/* chi-square test :

	it is non-parametric test that is performed on catagorical(i.e. nomianl or ordinal) data
	non parametric test is a test in which you test the hypothesis without taking any population parametres

	it is comparison between observed and expencted frequencies

	it also tests independeance od two variavbles

	sample size > 50

	degree of freedom = no. of catagories - 1

	(chi-square) χ² =   (Oi - Ei)²							--------Oi = observed value at position i , Ei	= expected value at position i
			  Σ ---------
			       Ei
	steps :
		1. define null and alternate hypothesis
		2. calculate alpha value and degree of freedom
		3. check decision boudry using chi-square table based on type of problem like one tail or 2 tail , get value with DF and alpha
		4. calculate chi-square (χ²)
		5. if χ² > decision boudry ,then reject null 
		6. if decision boudry > χ² , then accept null


/* U-test (Mann-Whitney ):

	used to investigate that 2 independent samples selected from same population have same distribution
	
	it is non-parametric counter part of t-test

	it is calculated as :
			   n1(n1 + 1)                        n2(n2 + 1)                      n is sample size and R is sum of ranks after giving rank to rech in sample
		U1 = R1 -  -------------    ,   U2 = R2 -  -------------   
				2                        	2


		U = U1 + U2 = n1*n2


/* H-test (kruscal-wallies ) :

	used to comparing 2 or more independent samples of various sizes , it is extends U test 

A/B testing is used to check which statistical models are better model A or B













