{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample data analysis notebook\n",
    "##### Steps Followed :\n",
    "###### 1. Importing the libraries and dataset\n",
    "###### 2. Exploring dataset\n",
    "###### 3. defining the features and target\n",
    "###### 4. deal with missing data\n",
    "###### 5. Encoding\n",
    "###### 6. Outliers\n",
    "###### 7. Correlation\n",
    "###### 8. Scaling\n",
    "###### 9. Spliting\n",
    "###### 10. apply machine learning algorithms\n",
    "###### 11. Test accuracy\n",
    "###### 12. Saving model and scaling features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul Agrawal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Rahul Agrawal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\Rahul Agrawal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "#####  get all details of the data like type of data, shape,count  etc.\n",
    "\n",
    "* ### also make un-necessory columns list and drop that columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking sample data\")\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" get detailed Info \")\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"differtiating data types\")\n",
    "obj = []\n",
    "others = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes == 'object':\n",
    "        obj.append(col)\n",
    "    else:\n",
    "        others.append(col)\n",
    "        \n",
    "print(\"object data types\")\n",
    "print(obj)\n",
    "print(\"***************************************************************************************\")\n",
    "print(\"int , float , boolean data types\")\n",
    "print(others)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dropping Un-necessory Columns\")\n",
    "\n",
    "#  Fill list here\n",
    "\n",
    "# col_to_be_dropped = []\n",
    "\n",
    "# df=df.drop(col_to_be_dropped,axis=1) \n",
    "# OR\n",
    "\n",
    "# for i in col_to_be_dropped:\n",
    "#     df.pop(i)   # using PoP method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = \"\"\n",
    "\n",
    "X = df.drop(target_col_name,axis=1)\n",
    "\n",
    "y= df[target_col_name]\n",
    "\n",
    "\n",
    "#   OR  \n",
    "\n",
    "\n",
    "# X = df.iloc[: , :-1]\n",
    "# y = df.iloc[: , -1]\n",
    "\n",
    "\n",
    "\n",
    "print(\"labels having shape : \",X.shape)\n",
    "print(\"Targets having shape : \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Unique Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting Unique values Count\")\n",
    "\n",
    "threshold = 10\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for col in df.columns:\n",
    "    if df[col].value_counts().count() < threshold:\n",
    "        print(f'column_name = \"{col}\" , Total_unique_values = [{df[col].value_counts().count()}]')\n",
    "        print(df[col].unique())\n",
    "        print(\"---------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  getting total no. possible combinations of 2 or more columns\n",
    "\n",
    "col_names_list = []\n",
    "\n",
    "comb = df.groupby(col_names_list).size().reset_index(name='Total_combinations_available')\n",
    "\n",
    "comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values\n",
    "#####  get the count of missing values in each column\n",
    "#####  on the basis of count we can decide whether to drop the column or not or just fill the missing values with mean or median or mode or any other value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking null values\")\n",
    "\n",
    "print(\"getting only those column having atleast one null value\")\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        print(f'column_name = {col} , Total_null_values = ',df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n",
    "\n",
    "\n",
    "s.set_xlabel('Column Names', fontsize=30)\n",
    "s.set_ylabel('Null Count', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dropping rows/touples having null values\")\n",
    "\n",
    "index_no = []\n",
    "\n",
    "col = \"col\"  #  enter column name here\n",
    "\n",
    "for i,j in enumerate(df[col].isnull()):\n",
    "    if(j==True):\n",
    "        index_no.append(i)\n",
    "        \n",
    "print(\"Row index that are going to be dropped are : \",index_no)\n",
    "\n",
    "# for i in index_no:\n",
    "#     df.drop(i , axis =0 , inplace = True)\n",
    "\n",
    "#  OR\n",
    "\n",
    "# df.dropna(inplace=True , axis=0)     # drop all rows having null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filling null records with a value \")\n",
    "\n",
    "#  Repeat this cell after changing column name\n",
    "\n",
    "col = \"col\"   #   Enter column name here\n",
    "\n",
    "value = df[col].mean()    # e.g. mean of the column\n",
    "\n",
    "df[col].fillna(value=value ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using Forward/backward fill method\")\n",
    "\n",
    "col = \"col\"\n",
    "\n",
    "df[col].ffill(axis = 0 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dropping duplicates\n",
    "\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Object data  \n",
    "#### Using Label Encoder  ( For ordinal data  like good,bad,etc))  \n",
    "##### * Using Map  ------     e.g. d[newcol]=d[col].map({'S':0,'Q':1,'C':2})\n",
    "##### * Using Replace    ------     e.g. df.replace(['female'],1,inplace=True)\n",
    "##### * Using Lambda     ------       e.g. df['new_col']=df['col_tobe_encoded'].apply(lambda x : 1 if x=='catagory1' 2 elif x=='catagory2' else 3)  \n",
    "##### * Using get_dummies  ------     e.g. df[new_col_name]=pd.get_dummies(df_train[col] , drop_first=True)   # only for 2 catagory columns\n",
    "##### * using sklearn lable encoder\n",
    "\n",
    "#### Using One Hot Encoder  ( For nominal data like pune,mumbai,etc)\n",
    "##### * Using get_dummies  ------     e.g. df=pd.get_dummies(df,columns=['col_tobe_encoded'])\n",
    "##### * Using sklearn onehotencoder   ------     \n",
    "\n",
    "* delete original col , if dummy cols are greater than 2 then also remove 1 col to reduce redundancy and also relieve model from dummy variable trap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"listing unique values For Encoding \")\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        if len(df[col].unique()) > 4:\n",
    "            print(f' ******  Warning  column_name = {col} has more than 4 unique values ******')\n",
    "        else:\n",
    "            print(f'column_name =\"{col}\", Unique_values = ',df[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder   # it is used to encode categorical data into numerical data like .map method\n",
    "\n",
    "le = LabelEncoder()\n",
    "dfle = df\n",
    "dfle[col] = le.fit_transform(dfle[col])\n",
    "dfle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For One Hot Encoding using get dummies\")    # it is used to encode categorical data into binary data with each catagory having its own column\n",
    "\n",
    "remove_first = False\n",
    "# remove_first = True\n",
    "\n",
    "df_backup = df     #in case if something fails\n",
    "\n",
    "col = \"col\"\n",
    "\n",
    "dummies = pd.get_dummies(df[col])   #  prefix=f\"{col}\"    #  it will add prefix to the column name as col_0, col_1, col_2\n",
    "\n",
    "\n",
    "\n",
    "df.drop([col], axis=1 , inplace = True)\n",
    "\n",
    "merged_df = pd.concat([df,dummies],axis='columns')\n",
    "\n",
    "if len(dummies.columns) > 3:\n",
    "    if(remove_first):\n",
    "        merged_df.drop([dummies.columns[0]], axis=1 , inplace = True)   # remove first column from dummies\n",
    "        print(\"Dropped first column from dummies i.e. \",dummies.columns[0])\n",
    "    else:\n",
    "        print(f' ******  Warning  Remove one of the Column from dummies to prevent to get in from dummies trap ******')\n",
    "\n",
    "\n",
    "merged_df.sample(10)\n",
    "\n",
    "\n",
    "# df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_to_drop = \"\"  # if warning is raised\n",
    "\n",
    "if col_to_drop !=\"\":\n",
    "    merged_df.drop([col_to_drop],axis=1,inplace=True)\n",
    "    \n",
    "df = merged_df\n",
    "\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"using Label binarizer\")\n",
    "\n",
    "col=  \"col\"\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "df1 = LabelBinarizer().fit_transform(df[col])     # df1 is array of 0 and 1 with shape (n_samples,n_classes)\n",
    "\n",
    "#  To convert back to original dataframe\n",
    "\n",
    "col_names = df[col].unique()\n",
    "\n",
    "if df1.shape[1] > 1:\n",
    "    df_temp = pd.DataFrame(df1 , columns=col_names)\n",
    "    col_dropping = df_temp.columns[len(df_temp.columns)-1]\n",
    "    print(f'Dropping column {col_dropping}')\n",
    "    df_temp.drop(col_dropping, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"getting column no. for onehotencoder\")\n",
    "\n",
    "# for index,col in enumerate(df.columns):\n",
    "#     print(f'At Index {index} is column \"{col}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For One Hot Encoding using sklearn OneHotEncoder\")     #  something is wrong in code\n",
    "from sklearn.preprocessing import OneHotEncoder      \n",
    "\n",
    "# col_no = 45\n",
    "\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# ct = OneHotEncoder(catagorical_features = [col_no])    # Enter Column no that has to be encoded\n",
    "# df = ct.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Data type of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  after encoding and filling null values , sometimes we need to change the data type of the column\n",
    "\n",
    "col = \"col\"\n",
    "\n",
    "df[col] = df[col].astype('int') #  change data type to int\n",
    "\n",
    "#  it is similar to\n",
    "# df[col] = df[col].apply(lambda x: int(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Outliers\n",
    "\n",
    "##### perform outlier detection using IQR and Boxplot and remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col = \"col\"\n",
    "\n",
    "sns.boxplot(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If Outlier detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df     #in case if something fails\n",
    "\n",
    "min_value =  0\n",
    "\n",
    "max_value = 0\n",
    "\n",
    "mini = np.where(df[col] < min_value ) \n",
    "mini = list(mini[0])\n",
    "\n",
    "maxi = np.where(df[col] > max_value )\n",
    "maxi =list(maxi[0])\n",
    "\n",
    "total_index = mini + maxi\n",
    "\n",
    "df.drop(total_index, axis =0 , inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Co-relation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix, annot=True , cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = \"col\"\n",
    "\n",
    "col2 = \"col\"\n",
    "\n",
    "print(f\"plotting scatter plot to visulaize the relationship between two columns i.e {col1} and {col2}\")\n",
    "\n",
    "sns.scatterplot(x=col1, y=col2, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data\n",
    "\n",
    "* ##### Choose scaling technique carefully\n",
    "* ##### save the scaler object for future use\n",
    "\n",
    "### Scaling techniques available\n",
    "\n",
    "#### 1. Standard scaling  =>  StandardScaler()\n",
    "#####   scales the values such that the mean is 0 and the standard deviation is 1  , x_scaled = x – mean/std_dev\n",
    "#####   not much effective if ouliers are present\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "#### 2. Min-Max Scaling => MinMaxScaler(feature_range=(0,1)) \n",
    "#####  x_scaled = (x – x_min)/(x_max – x_min)\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "#### 3. Robust scaling  => RobustScaler()\n",
    "##### scales the data by the InterQuartile Range(IQR)  ,   x_scaled = (x – Q1)/IQR   , IQR = Q3 – Q1\n",
    "#####  due to IQR it is robust against outliers\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "#### 4.MaxAbsScaler => MaxAbsScaler()\n",
    "##### takes the absolute maximum value of each column and divides each value in the column by the maximum value.\n",
    "##### it is sensitive to outliers\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "#### 5.Quantile Transformer Scaler => QuantileTransformer(output_distribution='normal')\n",
    "#####  converts the variable distribution to a normal distribution. it is best to use this for non-linear data\n",
    "#####\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "#### 6.Log Transformation\n",
    "##### used to convert a skewed distribution to a normal distribution/less-skewed distribution\n",
    "##### Reducing the impact of too-low and too-high values.\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "#### 7.Power Transformer Scaler => PowerTransformer(method = 'box-cox')\n",
    "##### Box-Cox works with only positive values, while Yeo-Johnson works with both positive and negative values.\n",
    "##### it also tries to form a normal distribution\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "#### 8.Unit Vector Scaler/Normalizer => Normalizer(norm = 'l2')\n",
    "##### Normalizer works on the rows\n",
    "##### If we are using L1 norm, the values in each column are converted so that the sum of their absolute values along the row = 1\n",
    "##### If we are using L2 norm, the values in each column are first squared and added so that the sum of their absolute values along the row = 1\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Importing Libraries\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import MaxAbsScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.preprocessing import QuantileTransformer\n",
    "# from sklearn.preprocessing import PowerTransformer\n",
    "# from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() \n",
    "\n",
    "scaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving scaling features for future use \")\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "name_of_file = \"model\"\n",
    "\n",
    "dump(scaler, open(f\"{name_of_file}_scaling_features.pkl\", 'wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correlation after scaling \")\n",
    "\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Removing columns that are not highly correlent with other columns\n",
    "\n",
    "val = df.corr().values\n",
    "cols_corr = list(df.corr().columns)\n",
    "\n",
    "counter = 0\n",
    "threshold = 0.5\n",
    "for i in range(len(val)):\n",
    "    reject = True\n",
    "    for index,j in enumerate(val[i]):\n",
    "        if index == i :\n",
    "            pass\n",
    "        else:\n",
    "            if abs(j) > threshold:\n",
    "                reject = False\n",
    "        \n",
    "    if (reject):\n",
    "        print(cols_corr[i] , \"can be removed\")\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(df , y , test_size = 0.3 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" X_train shape = {X_train.shape}\")\n",
    "print(f\" X_test shape = {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### The cross_val_score() function will be used to perform the evaluation, taking the dataset and cross-validation configuration and returning a list of scores calculated for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# mse = cross_val_score(estimator = regressor , X = X_train , y = y_train , scoring = 'neg_mean_squared_error' , cv = 10)\n",
    "\n",
    "# mse = cross_val_score(model_object , X_data , y_data , scoring = 'neg_mean_squared_error' , cv = 10)   # cv is cross validation value\n",
    "\n",
    "\n",
    "# mse contains list of mse values for each fold that is equal to cv value , in this case 10\n",
    "\n",
    "# mean_mse = np.mean(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error calculation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save the model\n",
    "print(\"saving model for future use \")\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "name_of_file = \"model\"\n",
    "\n",
    "dump(model, open(f\"{name_of_file}_learing_features.pkl\", 'wb'))\n",
    "\n",
    "    \n",
    "# Load Saved Model to test\n",
    "from pickle import load\n",
    "\n",
    "\n",
    "mp = load(open(f\"{name_of_file}_learing_features.pkl\", 'rb'))\n",
    "\n",
    "    \n",
    "if mp.intercept_ == model.intercept_ and mp.coef_ == model.coef_:\n",
    "    print(\"Saved Model Successfully\")\n",
    "else:\n",
    "    print(\"something went wrong , save model again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save the model\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(model, 'model_joblib')\n",
    "\n",
    "#  Load Saved Model\n",
    "\n",
    "mj = joblib.load('model_joblib')\n",
    "\n",
    "# Predict using the saved model\n",
    "\n",
    "if mj.intercept_ == model.intercept_ and mj.coef_ == model.coef_:\n",
    "    print(\"Saved Model Successfully\")\n",
    "else:\n",
    "    print(\"something went wrong , save model again\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbcae02c4273700af05fd878483d93fcaab7791578566c35fbbd6258eb8c5fe6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
