
Lifecycle

1. import libraires and dataset
2. gather metadata and object types , remove unwanted columns
3. deal with null values , fill depending upon data requirement
4. Encoding data to int or float
5. outlier detection , remove outlier or fill with measure of central tendancy







Required Libraries:

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns

********************************************************************************************************************************************************************
                                                    Loading Data
********************************************************************************************************************************************************************


df = pd.read_excel("sample.xlsx")     Or
all_df = pd.read_csv('titanic.csv')
saving csv file use df.to_csv('titanic_preprocessed.csv' , index=False) to avoid index problem 

get sample from df  == df.sample(3)   returns 3 sample randomly



********************************************************************************************************************************************************************
                                                               Listing Data    0 = column  And  1= Row
********************************************************************************************************************************************************************

## Get all 

df.info()             ---- Deatils about Dataframe like column name with data type , total notNull count

OR 

df.describe()       --- it will give details about mean , std , min , max , count , etc only if data type is numeric

---------------------------------------------------------

##  to list any sample data from dataframe upto N


df.sample(N)   

Also .head(N)     And   .tail(N)

---------------------------------------------------------

## get all columns name by 

df.columns

Or

for col in df:
    print(col)

---------------------------------------------------------


##  get only Unique Values from a column

df["col"].unique()


---------------------------------------------------------

##  List the unique values in a column and count the number of times they appear

df['col'].value_counts()       
Or
df.groupby('col_name').describe()        ---- it shows in more advance way with min max value , mean , std , etc


---------------------------------------------------------

##  get All values at Row N

df.values[2]                ----return array with list containing all values present at row 2

OR 

df.loc[2]                  ----return dataframe with all values present at row 2

df.loc[:50,COL_LIST]                ----return dataframe with all values present upto row 50 with columns COL_LIST , loc requires only list of columns
OR
df.iloc[:10 , : 5]          ----return dataframe with all values present at first 10 row and column 0 to 4 , iloc requires only interger values of columns


---------------------------------------------------------

##  Get Max , min values for a column (axis = 0 by default)

df["col"].max()       ----for obj datatype , it will return max value according to alphabetic order

df[col].mean()
df[col].mode()
df[col].median()

df.mean()    -- get mean for all col having dtype = float or int

---------------------------------------------------------


## Get datatype for a column

df["col"].dtype            --- returns numpy.dtype , o for object , int64 for int 

Get datatype for each column

for col in df:
    print(f'col={col} , Datatype=',df[col].dtypes)

---------------------------------------------------------
## List specific columns list

df[["col1","col2","col3"]]

---------------------------------------------------------

## listing dataframe with specific conditions

e.g. model from year 2015 onwards

df[df["model"]>=2015]

e.g. model from year 2015 onwards and price less than 150000

df[(df["year"]>=2015) & (df["selling_price"]<150000)]          ----more condition can be added with & and ()

---------------------------------------------------------
## modify dataframe values with using function

def fun(X):
    return X+1

df["col"] = df["col"].apply(fun)      --- update the value of column with function 

df["col"] = df["col"].apply(lambda x:x+1)      --- update the value of column with function

---------------------------------------------------------
---------------------------------------------------------


********************************************************************************************************************************************************************
                                                            Dropping Unnecessary Columns
********************************************************************************************************************************************************************


## make unwanted column list 

        droping_columns = []           ----insert unwanted column name here

df=df.drop(droping_columns,axis=1,inplace=True)        ----droping unwanted column in same dataframe only






********************************************************************************************************************************************************************
                                                            Deal With Null Values
********************************************************************************************************************************************************************

## List all null values in a column col 

df["col"].isnull()     ---- true for null value and false for non null value

OR

df[col].isnull().sum()  ---- return total number of null values in a column
df.isnull().sum()     --- for every column

e.g.
for col in df:     # to print columsn having more than 0 null values
    total=df[col].isnull().sum()
    if(total>0):
        print('col=',col,' Null values=',total )

# only returns boolean value true if there is more than zero null values

df.isnull.values.any()

# get row index of null touples

for i,j in enumerate(df[col].isnull()):
    if(j==True):
        print(i)


    //// Techniques to deal with missing value problem

## Drop the touple Or row having Null Value_count  , *remove entire column if max touples of a perticular column having null values

df.dropna(axis=0,inplace=True)

## Fill the null values with mean of column , we can take median as well as mode

df[col].fillna(value=df[col].mean() ,inplace=True)   #with mean 

OR

df[col].fillna(value='SAMPLE' ,inplace=True) # specific value or zero

// Forward Fill


# applying ffill() method to fill the missing values
df.ffill(axis = 0)      # only row wise filling 





********************************************************************************************************************************************************************
                                                    Outliers
********************************************************************************************************************************************************************

*************   get outliers using Boxplot *********

 outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error

# can be found using 

	sns.boxplot(df[col])   # it will plot a boxplot having minima , maxima , median and outlier points

# found out the index of that outlier using

	np.where(df[col] < value )    # value is considered using boxplot

# remove tha touple 

	df.drop(df[df[col] < value].index, inplace = True)   # it will that perticular row only


*************   get outliers using math formulas  *********

# get quantile range

	q1 = np.quantile(df[col], .25)

	q3 = np.quantile(df[col], .75)

	minimum = 


*************   get outliers using math formulas zscore *********

zscore = np.abs(stats.zscore(df['reading score']))


******************************************************************************************************************************************************************
                                                    		Encoding
********************************************************************************************************************************************************************


	For nominal data use dummies as pune > mumbai > kolkata doesnt make sence if encoded directly

		create dummy for a column :

			dummies = pd.get_dummies(df.col_name)

			# merge into original dataframe 

			merged_df = pd.concat([df,dummies],axis='columns')

			# drop the main column

			df.drop([col_name], axis=1 , inplace = true)

			# if dummies size > 2 , then  drop one of the dummy column to reduce redundancy and also relieve model from dummy variable trap

			# e.g. if for dummy C , drop C column then A=0 , B=0 assumed as it is value of C

	For Ordinal Data like good , avg , bad :

			# use replace method 

		
			



********************************************************************************************************************************************************************
                                                   Scaling
********************************************************************************************************************************************************************

		use sns.pairplot(df)   to see 

 1. Staderd scaler 
 
 
	scaler=StandardScaler()
	### fit vs fit_transform
	X_train_scaled=scaler.fit_transform(X_train)
	

 2. Minmax scaler 
 
 	mainly used in cnn to scale down 0 to 1 from 0-255
 	
 	min_max=MinMaxScaler()        # MinMaxScaler(feature_range=(0,1)       --- is by default
 	
 	
 3. Robust Scaler
	Robust Scaler are robust to outliers.It is used to scale the feature to median and quantiles Scaling using median and quantiles consists of substracting the median to all the observations, and then 		dividing by the interquantile difference. The interquantile difference is the difference between the 75th and 25th quantile:

	IQR = 75th quantile - 25th quantile

	X_scaled = (X - X.median) / IQR
	
	scaler=RobustScaler()
	
#### If you want to check whether feature is guassian or normal distributed
#### Q-Q plot
def plot_data(df,feature):
    plt.figure(figsize=(10,6))
    plt.subplot(1,2,1)
    df[feature].hist()
    plt.subplot(1,2,2)
    stat.probplot(df[feature],dist='norm',plot=pylab)
    plt.show()
    
	
 4. Guassian Transformation
	
	Some machine learning algorithms like linear and logistic assume that the features are normally distributed -Accuracy -Performance
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	




